
#' Initialize a model
#'
#' This function creates a list of word indexes W, topic indicators Z, seed indicators X,
#' a vocabulary list \code{vocab} from \code{files}.
#'
#' \code{Z} represents which topic a word topken was generated by.  It is initialized randomly
#' from \code{length(dict) + extra_k} topics, except when \code{dict}
#' assigns a word to a particular category, in which case this topic indicator is assigned.
#' For example, if a word appears in the second category of \code{dict} it will always be initialized
#' as 1.
#'
#' \code{X} is 1 when a word is generated from one of the topics from the dictionary ('seeded')
#' and 0 when it was generated from one of the \code{extra_k} normal topics.
#' \code{X} is initialized randomly except when the word is contained in \code{dict}.
#'
#' Note that all indicators are zero-based for ease of later processing in C++, so
#' \code{mod$vocab[mod$W + 1]} recovers the (tokenized) words of the i-th document from
#' model \code{mod}.
#'
#' @param files Names of files to process
#' @param dict a quanteda dictionary or named list of character vectors
#' @param extra_k number of unseeded topics in addition to the topics seeded by \code{dict}
#' @param encoding File encoding (defaults to \code{readLines}'s default)
#' @param ... Additional arguments to \code{quanteda::tokens}. (Only lowercasing happens by default).
#'
#' @return A list containing \itemize{
#'         \item{W}{a list of vectors of word indexes}
#'         \item{Z}{a list of vectors of topic indicators isomorphic to W},
#'         \item{X}{a list of vectors of seed indicators (0/1) isomorphic to W}
#'         \item{vocab}{a vector of vocabulary items}
#'         \item{files}{a vector of document filenames}
#'         \item{dict}{a tokenized version of the dictionary}
#'         \item{seeds}{a list of words for the seed words in dict, named by dictionary category}
#'         }.
#' @export
#'
init <- function(files, dict, extra_k = 1, encoding = "unknown", ...){
  K <- length(dict)

  tokenize_text <- function(x){
    x <- paste(readLines(x, encoding = encoding), collapse = "\n")
    as.character(quanteda::tokens(quanteda::char_tolower(x), ...))
  }
  # tokenize the dictionary entries and documents
  # Note: quanteda parameters (...) may lead to dropping some words
  dict <- lapply(dict, function(x){ as.character(quanteda::tokens(x)) })
  toklist <- lapply(files, tokenize_text)

  ## construct W and a vocab list (W elements are 0 based ids)
  vocab <- unique(unlist(toklist))
  wd_map <- hashmap::hashmap(vocab, as.integer(1:length(vocab) - 1))
  W <- lapply(toklist, function(x){ wd_map[[x]] })

  # zx_assigner maps seed words to category ids
  seed_wdids <- unlist(lapply(dict, function(x){ wd_map$find(x) }))
  cat_ids <- rep(1:K - 1, unlist(lapply(dict, length)))
  zx_assigner <- hashmap::hashmap(as.integer(seed_wdids), as.integer(cat_ids))

  make_x <- function(x){
    z <- zx_assigner$find(x) # if the word is a seed, 1, else 0
    as.numeric(!is.na(z))
  }
  X <- lapply(W, make_x)

  # if the word is a seed, assign the appropriate (0 starting) Z, else a random Z
  make_z <- function(x){
    zz <- zx_assigner$find(x)
    unseeded <- is.na(zz)
    zz[unseeded] <- sample((1:(K + extra_k)) - 1, sum(unseeded), replace = TRUE)
    zz
  }
  Z <- lapply(W, make_z)

  # dictionary category names -> vector of word_id. (Later processes ignore names)
  seeds <- lapply(dict, function(x){ wd_map[[x]] })

  list(W = W, Z = Z, X = X, vocab = vocab,
       files = files, dict = dict, seeds = seeds)
}





# train_seededlda <- function(files, dict, k, encoding = "unknown", iter = ...){
#   model <- init(files, dict, k, encoding = "unknown", ...)
#   train_model(List W, List X, List Z, List id_dict,
#     StringVector files, StringVector vocab,
#     int k_seeded, int k_free, double alpha_k,
#     int iter = 0)
#
# }
