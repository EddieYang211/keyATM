
#' Initialize a topicdict model
#'
#' This function creates a list of word indexes W, topic indicators Z,
#' seed indicators X, a vocabulary list \code{vocab} from \code{files}.
#'
#' \code{Z} represents which topic a word topken was generated by.
#' It is initialized randomly
#' from \code{length(dict) + extra_k} topics, except when \code{dict}
#' assigns a word to a particular category, in which case this topic
#' indicator is assigned.
#' For example, if a word appears in the second category of \code{dict}
#' it will always be initialized as 1.
#'
#' \code{X} is 1 when a word is generated from one of the topics from
#' the dictionary ('seeded') and 0 when it was generated from one of
#' the \code{extra_k} normal topics. \code{X} is initialized randomly
#' except when the word is contained in \code{dict}.
#'
#' Note that all indicators are zero-based for ease of later processing
#' in C++, so \code{mod$vocab[mod$W + 1]} recovers the (tokenized) words
#' of the i-th document from model \code{mod}.
#'
#' If \code{alpha} is a scalar this is the value given to all elements of
#' alpha. If it is a vector of the correct length those values are used
#' as the starting alphas.
#'
#' @param file_pattern name of a file or a wildcard, e.g. \code{"docs/*.txt"}
#' @param dict a quanteda dictionary or named list of character vectors
#' @param extra_k number of unseeded topics in addition to the topics seeded by
#'                \code{dict}
#' @param encoding File encoding (Default: whatever \code{quanteda} guesses)
#' @param lowercase whether to transform each token to lowercase letters
#' @param remove_numbers whether to remove numbers
#' @param remove_punct whether to remove punctuation
#' @param remove_symbols whether to remove non-alphanumerical symbols
#' @param remove_separators whether to remove tabs, spaces, newlines, etc.
#' @param remove_twitter whether to remove Twitter detritus
#' @param remove_hyphens whether to split hyphenated words
#' @param remove_url whether to remove URLs
#' @param stem_language if not NULL, the language to use for stemming
#' @param stopwords if not NULL, a character vector of words to remove,
#'                  e.g. \code{quanteda::stopwords("english")}
#' @param alpha Starting value for all the model's topic proportion hyperparameters (default: 50 / number of topics)
#'
#' @return A list containing \describe{
#'         \item{W}{a list of vectors of word indexes}
#'         \item{Z}{a list of vectors of topic indicators isomorphic to W},
#'         \item{X}{a list of vectors of seed indicators (0/1) isomorphic to W}
#'         \item{vocab}{a vector of vocabulary items}
#'         \item{files}{a vector of document filenames}
#'         \item{dict}{a tokenized version of the dictionary}
#'         \item{seeds}{a list of words for the seed words in dict, named by dictionary category}
#'         \item{alpha}{a vector of topic proportion hyperparameters}
#'         \item{call}{details of the function call}
#'         }.
#' @importFrom quanteda corpus docvars tokens tokens_tolower tokens_remove tokens_wordstem dictionary
#' @importFrom readtext readtext
#' @importFrom hashmap hashmap
#' @export
topicdict_model <- function(file_pattern, dict, extra_k = 1, encoding = NULL,
                            lowercase = TRUE,
                            remove_numbers = TRUE, remove_punct = TRUE,
                            remove_symbols = TRUE, remove_separators = TRUE,
                            remove_twitter = FALSE, remove_hyphens = FALSE,
                            remove_url = TRUE, stem_language = NULL,
                            stopwords = NULL,
                            alpha = 50/(length(dict)+extra_k)){
  cl <- match.call()

  proper_len <- length(dict) + extra_k
  if (length(alpha) == 1){
    message("All ", proper_len, " values for alpha starting as ", alpha)
    alpha = rep(alpha, proper_len)
  } else if (length(alpha) != proper_len)
    stop("Starting alpha must be a scalar or a vector of length ", proper_len)

  args <- list(remove_numbers = remove_numbers,
               remove_punct = remove_punct,
               remove_symbols = remove_symbols,
               remove_separators = remove_separators,
               remove_twitter = remove_twitter,
               remove_hyphens = remove_hyphens,
               remove_url = remove_url)

  ## preprocess each text
  args$x <- corpus(readtext(file_pattern, encoding = encoding))
  doc_names <- docvars(args$x, "doc_id") # docnames
  toks <- do.call(tokens, args = args)
  if (lowercase)
    toks <- tokens_tolower(toks)
  if (!is.null(stopwords))
    toks <- tokens_remove(toks, stopwords)
  if (!is.null(stem_language))
    toks <- tokens_wordstem(toks, language = stem_language)

  ## apply the same preprocessing to the seed words
  args$x <- do.call(rbind, lapply(as.list(dict), paste0, collapse = " "))
  dtoks <- do.call(tokens, args = args)
  if (lowercase)
    dtoks <- tokens_tolower(dtoks)
  if (!is.null(stopwords))
    dtoks <- tokens_remove(dtoks, stopwords)
  if (!is.null(stem_language))
    dtoks <- tokens_wordstem(dtoks, language = stem_language)
  K <- length(dtoks) # number of seeded categories a.k.a. size of dictionary

  ## construct W and a vocab list (W elements are 0 based ids)
  wd_names <- attr(toks, "types") # vocab
  wd_map <- hashmap(wd_names, as.integer(1:length(wd_names) - 1))
  W <- lapply(toks, function(x){ wd_map[[x]] })

  # zx_assigner maps seed words to category ids
  seed_wdids <- unlist(lapply(dtoks, function(x){ wd_map$find(x) }))
  cat_ids <- rep(1:K - 1, unlist(lapply(dtoks, length)))
  zx_assigner <- hashmap(as.integer(seed_wdids), as.integer(cat_ids))

	## xx indicates whether the word comes from a seed topic-word distribution or not
	make_x <- function(x){
    seeded <- as.numeric(zx_assigner$has_keys(x)) # 1 if they're a seed
		# Use x structure
    x[seeded == 0] <- 0 # non-seeded words have x=0
		x[seeded == 1] <- sample(0:1, length(x[seeded == 1]), prob = c(0.3, 0.7), replace = TRUE)
			# seeded words have x=1 probabilistically
    x
	}

  X <- lapply(W, make_x)

  # if the word is a seed, assign the appropriate (0 start) Z, else a random Z
  make_z <- function(x){
		zz <- zx_assigner[[x]] # if it is a seed word, we already know the topic
		zz[is.na(zx_assigner[[x]])] <- sample(1:(K + extra_k) - 1,
																					sum(as.numeric(is.na(zx_assigner[[x]]))),
																					replace = TRUE)
    zz
  }
  Z <- lapply(W, make_z)

  # dictionary category names -> vector of word_id.
  # (Later processes ignore names)
  seeds <- lapply(dtoks, function(x){ wd_map$find(x) })

  ll <- list(W = W, Z = Z, X = X, vocab = wd_names,
             files = doc_names, dict = dtoks, seeds = seeds,
             extra_k = extra_k,
             alpha = alpha, call = cl)
  class(ll) <- c("topicdict", class(ll))
  ll
}


# train_seededlda <- function(files, dict, k, encoding = "unknown", iter = ...){
#   model <- init(files, dict, k, encoding = "unknown", ...)
#   train_model(List W, List X, List Z, List id_dict,
#     StringVector files, StringVector vocab,
#     int k_seeded, int k_free, double alpha_k,
#     int iter = 0)
#
# }
