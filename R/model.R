
#' Initialize a topicdict model
#'
#' This function creates a list of word indexes W, topic indicators Z,
#' seed indicators X, a vocabulary list \code{vocab} from \code{files}.
#'
#' \code{Z} represents which topic a word topken was generated by.
#' It is initialized randomly
#' from \code{length(dict) + extra_k} topics, except when \code{dict}
#' assigns a word to a particular category, in which case this topic
#' indicator is assigned.
#' For example, if a word appears in the second category of \code{dict}
#' it will always be initialized as 1.
#'
#' \code{X} is 1 when a word is generated from one of the topics from
#' the dictionary ('seeded') and 0 when it was generated from one of
#' the \code{extra_k} normal topics. \code{X} is initialized randomly
#' except when the word is contained in \code{dict}.
#'
#' Note that all indicators are zero-based for ease of later processing
#' in C++, so \code{mod$vocab[mod$W + 1]} recovers the (tokenized) words
#' of the i-th document from model \code{mod}.
#'
#' If \code{alpha} is a scalar this is the value given to all elements of
#' alpha. If it is a vector of the correct length those values are used
#' as the starting alphas.
#'
#' @param file_pattern name of a file or a wildcard, e.g. \code{"docs/*.txt"}
#' @param dict a quanteda dictionary or named list of character vectors
#' @param extra_k number of unseeded topics in addition to the topics seeded by
#'                \code{dict}
#' @param encoding File encoding (Default: whatever \code{quanteda} guesses)
#' @param lowercase whether to transform each token to lowercase letters
#' @param remove_numbers whether to remove numbers
#' @param remove_punct whether to remove punctuation
#' @param remove_symbols whether to remove non-alphanumerical symbols
#' @param remove_separators whether to remove tabs, spaces, newlines, etc.
#' @param remove_twitter whether to remove Twitter detritus
#' @param remove_hyphens whether to split hyphenated words
#' @param remove_url whether to remove URLs
#' @param stem_language if not NULL, the language to use for stemming
#' @param stopwords if not NULL, a character vector of words to remove,
#'                  e.g. \code{quanteda::stopwords("english")}
#' @param alpha Starting value for all the model's topic proportion hyperparameters (default: 50 / number of topics)
#'
#' @return A list containing \describe{
#'         \item{W}{a list of vectors of word indexes}
#'         \item{Z}{a list of vectors of topic indicators isomorphic to W},
#'         \item{X}{a list of vectors of seed indicators (0/1) isomorphic to W}
#'         \item{vocab}{a vector of vocabulary items}
#'         \item{files}{a vector of document filenames}
#'         \item{dict}{a tokenized version of the dictionary}
#'         \item{seeds}{a list of words for the seed words in dict, named by dictionary category}
#'         \item{extra_k}{how many extra non-seeded topics are required}
#'         \item{alpha}{a vector of topic proportion hyperparameters}
#'         \item{alpha_iter}{a list to store topic proportion hyperparameters}
#'         \item{model_fit}{a list to store perplexity and log-likelihood}
#'         \item{gamma1}{First prior probability parameter for X (currently the same for all topics)}
#'         \item{gamma2}{Second prior probability parameter for X (currently the same for all topics)}
#'         \item{beta}{prior parameter for the non-seeded word generation probabilities}
#'         \item{beta_s}{prior parameter for the seeded word generation probabilities}
#'         \item{call}{details of the function call}
#'         }.
#' @importFrom quanteda corpus docvars tokens tokens_tolower tokens_remove tokens_wordstem dictionary
#' @importFrom readtext readtext
#' @importFrom hashmap hashmap
#' @export
topicdict_model <- function(file_pattern, dict, extra_k = 1, encoding = NULL,
                            lowercase = TRUE,
                            remove_numbers = TRUE, remove_punct = TRUE,
                            remove_symbols = TRUE, remove_separators = TRUE,
                            remove_twitter = FALSE, remove_hyphens = FALSE,
                            remove_url = TRUE, stem_language = NULL,
                            stopwords = NULL,
                            alpha = 50/(length(dict) + extra_k),
                            beta = 0.01, beta_s = 0.1,
                            gamma_1 = 1.0, gamma_2 = 1.0){
  cl <- match.call()

  proper_len <- length(dict) + extra_k
  if (length(alpha) == 1){
    message("All ", proper_len, " values for alpha starting as ", alpha)
    alpha = rep(alpha, proper_len)
  } else if (length(alpha) != proper_len)
    stop("Starting alpha must be a scalar or a vector of length ", proper_len)

  args <- list(remove_numbers = remove_numbers,
               remove_punct = remove_punct,
               remove_symbols = remove_symbols,
               remove_separators = remove_separators,
               remove_twitter = remove_twitter,
               remove_hyphens = remove_hyphens,
               remove_url = remove_url)

  ## preprocess each text
  args$x <- corpus(readtext(file_pattern, encoding = encoding))
  doc_names <- docvars(args$x, "doc_id") # docnames
  toks <- do.call(tokens, args = args)
  if (lowercase)
    toks <- tokens_tolower(toks)
  if (!is.null(stopwords))
    toks <- tokens_remove(toks, stopwords)
  if (!is.null(stem_language))
    toks <- tokens_wordstem(toks, language = stem_language)

  ## apply the same preprocessing to the seed words
  args$x <- do.call(rbind, lapply(as.list(dict), paste0, collapse = " "))
  dtoks <- do.call(tokens, args = args)
  if (lowercase)
    dtoks <- tokens_tolower(dtoks)
  if (!is.null(stopwords))
    dtoks <- tokens_remove(dtoks, stopwords)
  if (!is.null(stem_language))
    dtoks <- tokens_wordstem(dtoks, language = stem_language)
  K <- length(dtoks) # number of seeded categories a.k.a. size of dictionary

  ## construct W and a vocab list (W elements are 0 based ids)
  wd_names <- attr(toks, "types") # vocab
  wd_map <- hashmap(wd_names, as.integer(1:length(wd_names) - 1))
  W <- lapply(toks, function(x){ wd_map[[x]] })

  # zx_assigner maps seed words to category ids
  seed_wdids <- unlist(lapply(dtoks, function(x){ wd_map$find(x) }))
  cat_ids <- rep(1:K - 1, unlist(lapply(dtoks, length)))
  zx_assigner <- hashmap(as.integer(seed_wdids), as.integer(cat_ids))

	## xx indicates whether the word comes from a seed topic-word distribution or not
	make_x <- function(x){
    seeded <- as.numeric(zx_assigner$has_keys(x)) # 1 if they're a seed
		# Use x structure
    x[seeded == 0] <- 0 # non-seeded words have x=0
		x[seeded == 1] <- sample(0:1, length(x[seeded == 1]), prob = c(0.3, 0.7), replace = TRUE)
			# seeded words have x=1 probabilistically
    x
	}

  X <- lapply(W, make_x)

  # if the word is a seed, assign the appropriate (0 start) Z, else a random Z
  make_z <- function(x){
		zz <- zx_assigner[[x]] # if it is a seed word, we already know the topic
		zz[is.na(zx_assigner[[x]])] <- sample(1:(K + extra_k) - 1,
																					sum(as.numeric(is.na(zx_assigner[[x]]))),
																					replace = TRUE)
    zz
  }
  Z <- lapply(W, make_z)

  # dictionary category names -> vector of word_id.
  # (Later processes ignore names)
  seeds <- lapply(dtoks, function(x){ wd_map$find(x) })
  names(seeds) <- names(dict)

  ll <- list(W = W, Z = Z, X = X, vocab = wd_names,
             files = doc_names, dict = dtoks, seeds = seeds, extra_k = extra_k,
             alpha = alpha, gamma_1 = gamma_1, gamma_2 = gamma_2,
             beta = beta, beta_s = beta_s, call = cl,
						 alpha_iter = list(), model_fit = list(),
						 call = cl)
  class(ll) <- c("topicdict", class(ll))
  ll
}

#' A Reference Class to explore documents 
#'
#' Explore Documents Class
#'
#' @docType class
#'
#' @section Fields:
#'  \describe{
#'    \item{\code{data}}{ data in tidytext format}
#'    \item{\code{uniquewords}}{ a vector of unique words}
#'    \item{\code{totalwords}}{ number of words in the entire documents}
#'  }
#'
#'
#' @section Methods:
#'  \describe{
#'    \item{\code{initialize}}{ Constructor}
#'    \item{\code{check_existence}}{ Check whether the word exists in the corpus}
#'    \item{\code{show_words(word, type="count", n_show=100, xaxis=F)}}{ show a word distribution. \code{type} should be \code{count} or \code{proportion}.}
#'    \item{\code{top_words(n_show=10)}}{ show top words}
#'  }
#'
#' @importFrom tidytext tidy 
#' @import ggplot2 
#' @import dplyr 
#' @export ExploreDocuments
#' @exportClass ExploreDocuments
ExploreDocuments <- setRefClass(
	Class = "ExploreDocuments",

	fields = list(
		data = "data.frame",
		uniquewords = "character",
		totalwords = "numeric"
	),

	methods = list(
		initialize = function(data_=tidy_){
			data_ <- data_ %>%
				group_by(document) %>%
				mutate(countdoc = sum(count)) %>%
				ungroup()
			data <<- data_
			uniquewords <<- unique(data$term)
			totalwords <<- sum(data$count)
		},

		check_existence = function(word){
			if(! (word %in% uniquewords)){
				message(paste0("'", word, "' is not in a corpus"))
				return (1)
			}else{
				return (0)
			}
		},

		show_words = function(word, type="count", n_show=100, xaxis=F){
			"Visualize a word distribution" 
			# check the words existence						
			c <- check_existence(word)
			if(c){ return()}

			# Visualize
			data %>% filter(term==get("word")) %>%
				summarize(total = sum(count)) %>% as.numeric() -> sumcount
			message(paste0("Count of '", word, "': ", as.character(sumcount), " out of ", totalwords))
			message(paste0("Proportion of '", word, "': ", as.character(round(sumcount/totalwords, 4))))

			data %>% 
				filter(term == get("word")) %>%
				group_by(document) %>%
				mutate(Proportion = count/countdoc*100) %>%
				ungroup() -> temp

			if(type=="count"){
				temp <- temp %>% top_n(n_show, count) %>% mutate(focus=count)
			}else{
				temp <- temp %>% top_n(get("n_show"), Proportion) %>% mutate(focus=Proportion)
			}

			p <- ggplot(temp, aes(x=reorder(document, -focus), y=focus)) +
				geom_bar(stat="identity")

			p <- p + ggtitle(paste0("Distribution of '", word, "' across documents")) + 
				theme_bw() 

			if(type=="count"){
				p <- p + ylab("Count")
			}else{
				p <- p + ylab("Proportion")
			}

			if(!xaxis){
				p <- p + xlab("Documents") +
										theme(axis.text.x=element_blank(),
											axis.ticks.x=element_blank(),
											plot.title = element_text(hjust = 0.5))
			}else{
				p <- p + xlab("Documents")
				p <- p + theme(axis.text.x = element_text(angle = 90, hjust = 1),
											 plot.title = element_text(hjust = 0.5))
			}

			return(p)

		},

		top_words = function(n_show=20){
			"Show frequent words" 
				data %>%
					mutate(Word = term) %>%
					select(-term) %>%
					group_by(Word) %>%
					summarize(WordCount = sum(count)) %>%
					ungroup() %>%
					mutate(`Proportion(%)` = round(WordCount/totalwords*100, 3)) %>%
					top_n(n_show, WordCount) %>%
					arrange(-WordCount) %>%
					print(n=nrow(.))

		}
	)
)


#' Explore Documents 
#'
#' Explore the documents.
#'
#' @param file_pattern name of a file or a wildcard, e.g. \code{"docs/*.txt"}
#' @param encoding File encoding (Default: whatever \code{quanteda} guesses)
#' @param lowercase whether to transform each token to lowercase letters
#' @param remove_numbers whether to remove numbers
#' @param remove_punct whether to remove punctuation
#' @param remove_symbols whether to remove non-alphanumerical symbols
#' @param remove_separators whether to remove tabs, spaces, newlines, etc.
#' @param remove_twitter whether to remove Twitter detritus
#' @param remove_hyphens whether to split hyphenated words
#' @param remove_url whether to remove URLs
#' @param stem_language if not NULL, the language to use for stemming
#' @param stopwords if not NULL, a character vector of words to remove,
#'                  e.g. \code{quanteda::stopwords("english")}
#'
#' @return A R4 objects \describe{
#'         \item{show_words(word, type="count", n_show=100, xaxis=F)}{ show a word distribution. \code{type} should be \code{count} or \code{proportion}.}
#'         \item{top_words(n_show=10)}{ show top words}
#'         }.
#' @importFrom quanteda corpus docvars tokens tokens_tolower tokens_remove tokens_wordstem dictionary dfm
#' @importFrom readtext readtext
#' @importFrom tidytext tidy 
#' @import ggplot2 
#' @import methods
#' @import dplyr 
#' @export
explore <- function(file_pattern, encoding = NULL,
                            lowercase = TRUE,
                            remove_numbers = TRUE, remove_punct = TRUE,
                            remove_symbols = TRUE, remove_separators = TRUE,
                            remove_twitter = FALSE, remove_hyphens = FALSE,
                            remove_url = TRUE, stem_language = NULL,
                            stopwords = NULL){

  args <- list(remove_numbers = remove_numbers,
               remove_punct = remove_punct,
               remove_symbols = remove_symbols,
               remove_separators = remove_separators,
               remove_twitter = remove_twitter,
               remove_hyphens = remove_hyphens,
               remove_url = remove_url)

  ## preprocess each text
  args$x <- corpus(readtext(file_pattern, encoding = encoding))
  doc_names <- docvars(args$x, "doc_id") # docnames
  toks <- do.call(tokens, args = args)
  if (lowercase)
    toks <- tokens_tolower(toks)
  if (!is.null(stopwords))
    toks <- tokens_remove(toks, stopwords)
  if (!is.null(stem_language))
    toks <- tokens_wordstem(toks, language = stem_language)

	dfm_ <- dfm(toks)
	tidy_ <- tidy(dfm_)

	res <- ExploreDocuments$new(tidy_)

	return(res)

}
