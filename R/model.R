
#' Initialize a topicdict model
#'
#' This function creates a list of word indexes W, topic indicators Z,
#' seed indicators X, a vocabulary list \code{vocab} from \code{files}.
#'
#' \code{Z} represents which topic a word topken was generated by.
#' It is initialized randomly
#' from \code{length(dict) + extra_k} topics, except when \code{dict}
#' assigns a word to a particular category, in which case this topic
#' indicator is assigned.
#' For example, if a word appears in the second category of \code{dict}
#' it will always be initialized as 1.
#'
#' \code{X} is 1 when a word is generated from one of the topics from
#' the dictionary ('seeded') and 0 when it was generated from one of
#' the \code{extra_k} normal topics. \code{X} is initialized randomly
#' except when the word is contained in \code{dict}.
#'
#' Note that all indicators are zero-based for ease of later processing
#' in C++, so \code{mod$vocab[mod$W + 1]} recovers the (tokenized) words
#' of the i-th document from model \code{mod}.
#'
#' If \code{alpha} is a scalar this is the value given to all elements of
#' alpha. If it is a vector of the correct length those values are used
#' as the starting alphas.
#'
#' @param files names of each file to read
#' @param dict a quanteda dictionary or named list of character vectors
#' @param extra_k number of unseeded topics in addition to the topics seeded by
#'                \code{dict}
#' @param encoding File encoding (Default: whatever \code{quanteda} guesses)
#' @param lowercase whether to transform each token to lowercase letters
#' @param remove_numbers whether to remove numbers
#' @param remove_punct whether to remove punctuation
#' @param remove_symbols whether to remove non-alphanumerical symbols
#' @param remove_separators whether to remove tabs, spaces, newlines, etc.
#' @param remove_twitter whether to remove Twitter detritus
#' @param remove_hyphens whether to split hyphenated words
#' @param remove_url whether to remove URLs
#' @param stem_language if not NULL, the language to use for stemming
#' @param stopwords if not NULL, a character vector of words to remove,
#'                  e.g. \code{quanteda::stopwords("english")}
#' @param alpha Starting value for all the model's topic proportion hyperparameters (default: 50 / number of topics)
#'
#' @return A list containing \describe{
#'         \item{W}{a list of vectors of word indexes}
#'         \item{Z}{a list of vectors of topic indicators isomorphic to W},
#'         \item{X}{a list of vectors of seed indicators (0/1) isomorphic to W}
#'         \item{vocab}{a vector of vocabulary items}
#'         \item{files}{a vector of document filenames}
#'         \item{dict}{a tokenized version of the dictionary}
#'         \item{seeds}{a list of words for the seed words in dict, named by dictionary category}
#'         \item{extra_k}{how many extra non-seeded topics are required}
#'         \item{alpha}{a vector of topic proportion hyperparameters}
#'         \item{alpha_iter}{a list to store topic proportion hyperparameters}
#'         \item{model_fit}{a list to store perplexity and log-likelihood}
#'         \item{gamma1}{First prior probability parameter for X (currently the same for all topics)}
#'         \item{gamma2}{Second prior probability parameter for X (currently the same for all topics)}
#'         \item{beta}{prior parameter for the non-seeded word generation probabilities}
#'         \item{beta_s}{prior parameter for the seeded word generation probabilities}
#'         \item{call}{details of the function call}
#'         }.
#' @importFrom quanteda corpus docvars tokens tokens_tolower tokens_remove tokens_wordstem dictionary
#' @importFrom hashmap hashmap
#' @export
topicdict_model <- function(files, dict, extra_k = 1, encoding = "UTF-8",
                            lowercase = TRUE,
                            remove_numbers = TRUE, remove_punct = TRUE,
                            remove_symbols = TRUE, remove_separators = TRUE,
                            remove_twitter = FALSE, remove_hyphens = FALSE,
                            remove_url = TRUE, stem_language = NULL,
                            stopwords = NULL,
                            alpha = 50/(length(dict) + extra_k),
                            beta = 0.01, beta_s = 0.1,
                            gamma_1 = 1.0, gamma_2 = 1.0){
  cl <- match.call()

  proper_len <- length(dict) + extra_k
  if (length(alpha) == 1){
    message("All ", proper_len, " values for alpha starting as ", alpha)
    alpha = rep(alpha, proper_len)
  } else if (length(alpha) != proper_len)
    stop("Starting alpha must be a scalar or a vector of length ", proper_len)

  args <- list(remove_numbers = remove_numbers,
               remove_punct = remove_punct,
               remove_symbols = remove_symbols,
               remove_separators = remove_separators,
               remove_twitter = remove_twitter,
               remove_hyphens = remove_hyphens,
               remove_url = remove_url)

  ## preprocess each text
  ## for debugging
	# Use files <- list.files(doc_folder, pattern="txt", full.names=T) when you pass
  df <- data.frame(text = unlist(lapply(files, function(x){ paste0(readLines(x, encoding = encoding),
                                                                collapse = "\n") })),
                   stringsAsFactors = FALSE)
  df$doc_id <- paste0("text", 1:nrow(df))
  args$x <- corpus(df)
  ## debugging until here

  # args$x <- corpus(readtext(file_pattern, encoding = encoding))
  doc_names <- docvars(args$x, "doc_id") # docnames
  toks <- do.call(tokens, args = args)
  if (lowercase)
    toks <- tokens_tolower(toks)
  if (!is.null(stopwords))
    toks <- tokens_remove(toks, stopwords)
  if (!is.null(stem_language))
    toks <- tokens_wordstem(toks, language = stem_language)

  ## apply the same preprocessing to the seed words
  args$x <- do.call(rbind, lapply(as.list(dict), paste0, collapse = " "))
  dtoks <- do.call(tokens, args = args)
  if (lowercase)
    dtoks <- tokens_tolower(dtoks)
  if (!is.null(stopwords))
    dtoks <- tokens_remove(dtoks, stopwords)
  if (!is.null(stem_language))
    dtoks <- tokens_wordstem(dtoks, language = stem_language)
  K <- length(dtoks) # number of seeded categories a.k.a. size of dictionary

  ## construct W and a vocab list (W elements are 0 based ids)
  wd_names <- attr(toks, "types") # vocab
  wd_map <- hashmap(wd_names, as.integer(1:length(wd_names) - 1))
  W <- lapply(toks, function(x){ wd_map[[x]] })

  # zx_assigner maps seed words to category ids
  seed_wdids <- unlist(lapply(dtoks, function(x){ wd_map$find(x) }))
  cat_ids <- rep(1:K - 1, unlist(lapply(dtoks, length)))
  zx_assigner <- hashmap(as.integer(seed_wdids), as.integer(cat_ids))

	## xx indicates whether the word comes from a seed topic-word distribution or not
	make_x <- function(x){
    seeded <- as.numeric(zx_assigner$has_keys(x)) # 1 if they're a seed
		# Use x structure
    x[seeded == 0] <- 0 # non-seeded words have x=0
		x[seeded == 1] <- sample(0:1, length(x[seeded == 1]), prob = c(0.3, 0.7), replace = TRUE)
			# seeded words have x=1 probabilistically
    x
	}

  X <- lapply(W, make_x)

  # if the word is a seed, assign the appropriate (0 start) Z, else a random Z
  make_z <- function(x){
		zz <- zx_assigner[[x]] # if it is a seed word, we already know the topic
		zz[is.na(zx_assigner[[x]])] <- sample(1:(K + extra_k) - 1,
																					sum(as.numeric(is.na(zx_assigner[[x]]))),
																					replace = TRUE)
    zz
  }
  Z <- lapply(W, make_z)

  # dictionary category names -> vector of word_id.
  # (Later processes ignore names)
  seeds <- lapply(dtoks, function(x){ wd_map$find(x) })
  names(seeds) <- names(dict)

  ll <- list(W = W, Z = Z, X = X, vocab = wd_names,
             files = doc_names, dict = dtoks, seeds = seeds, extra_k = extra_k,
             alpha = alpha, gamma_1 = gamma_1, gamma_2 = gamma_2,
             beta = beta, beta_s = beta_s, call = cl,
						 alpha_iter = list(), model_fit = list(),
						 call = cl)
  class(ll) <- c("topicdict", class(ll))
  ll
}

#' A Reference Class to explore documents
#'
#' Explore Documents Class
#'
#' @docType class
#'
#' @section Fields:
#'  \describe{
#'    \item{\code{data}}{ data in tidytext format}
#'    \item{\code{data_tfidf}}{ data in tidytext format}
#'    \item{\code{uniquewords}}{ a vector of unique words}
#'    \item{\code{num_uniquewords}}{ a number of unique words}
#'    \item{\code{totalwords}}{ number of words in the entire documents}
#'  }
#'
#'
#' @section Methods:
#'  \describe{
#'    \item{\code{initialize}}{ Constructor}
#'    \item{\code{check_existence}}{ Check whether the word exists in the corpus}
#'    \item{\code{visualize_dict_proportion}}{ Dictionary proportion list}
#'    \item{\code{visualize_words_documents(word, type="count", n_show=100, xaxis=F)}}{ show a word distribution. \code{type} should be \code{count} or \code{proportion}.}
#'    \item{\code{visualize_words(word, n_show=1:num_uniquewords)}}{ show a vector of words distribution.}
#'    \item{\code{top_words(n_show=10)}}{ show top words}
#'    \item{\code{visualize_tfidf(n_show=10)}}{ show top tf_idf}
#'    \item{\code{words_distribution(n_show)}}{ show a distribution of words}
#'  }
#'
#' @importFrom tidytext tidy bind_tf_idf
#' @import ggplot2
#' @import ggrepel
#' @import dplyr
#' @export ExploreDocuments
#' @exportClass ExploreDocuments
ExploreDocuments <- setRefClass(
	Class = "ExploreDocuments",

	fields = list(
		data = "data.frame",
		data_tfidf = "data.frame",
		uniquewords = "character",
		num_uniquewords = "numeric",
		totalwords = "numeric"
	),

	methods = list(
		initialize = function(data_=tidy_){
			data_tfidf <<- data_ %>%
				group_by(document) %>%
				mutate(countdoc = sum(count)) %>%
				ungroup() %>%
				bind_tf_idf(term, document, count)

			uniquewords <<- unique(data_$term)
			num_uniquewords <<- length(uniquewords)
			totalwords <<- sum(data_$count)

			data_ %>%
				mutate(Word = term) %>%
				select(-term) %>%
				group_by(Word) %>%
				summarize(WordCount = sum(count)) %>%
				ungroup() %>%
				mutate(`Proportion(%)` = round(WordCount/totalwords*100, 3)) %>%
				arrange(desc(WordCount)) %>%
				mutate(Ranking = 1:n()) ->> data
		},

		check_existence = function(word){
			if(! (word %in% uniquewords)){
				message(paste0("'", word, "' is not in a corpus"))
				return (1)
			}else{
				return (0)
			}
		},

		visualize_dict_prop = function(seed_list){
			names(seed_list) <- paste0("EstTopic", 1:length(seed_list))
			seeds <- lapply(seed_list, function(x){unlist(strsplit(x," "))})
			ext_k <- length(seeds)
			max_num_words <- max(unlist(lapply(seeds, function(x){length(x)})))

			seeds_df <- data.frame(EstTopic=1, Word=1)
			for(k in 1:ext_k){
				words <- seeds[[k]]
				numwords <- length(words)
				topicname <- paste0("EstTopic", k)
				for(w in 1:numwords){
					seeds_df <- rbind(seeds_df, data.frame(EstTopic=topicname, Word=words[w]))
				}
			}
			seeds_df <- seeds_df[2:nrow(seeds_df), ]

			inner_join(data, seeds_df, by="Word") %>%
				group_by(EstTopic) %>%
				mutate(Ranking = 1:n()) -> temp

			p <- ggplot(temp, aes(x=Ranking, y=`Proportion(%)`, colour=EstTopic)) +
				geom_line() +
				geom_point() +
				geom_label_repel(aes(label = Word), size=2.5,
												 box.padding = 0.20, label.padding = 0.12,
												 arrow=arrow(angle=10, length = unit(0.10, "inches"), ends = "last", type = "closed"),
												 show.legend = F) +
			  scale_x_continuous(breaks=1:max_num_words) +
				ylab("Proportion (%)") +
				theme_bw()

			return(p)

		},

		visualize_words_documents = function(word, type="count", n_show=100, xaxis=F){
			"Visualize a word distribution" 
			# check the words existence						
			c <- check_existence(word)
			if(c){ return()}

			# Visualize
			data_tfidf %>% filter(term==get("word")) %>%
				summarize(total = sum(count)) %>% as.numeric() -> sumcount
			message(paste0("Count of '", word, "': ", as.character(sumcount), " out of ", totalwords))
			message(paste0("Proportion of '", word, "': ", as.character(round(sumcount/totalwords, 4))))

			data_tfidf %>%
				filter(term == get("word")) %>%
				group_by(document) %>%
				mutate(Proportion = count/countdoc*100) %>%
				ungroup() -> temp

			if(type=="count"){
				temp <- temp %>% top_n(n_show, count) %>% mutate(focus=count)
			}else{
				temp <- temp %>% top_n(get("n_show"), Proportion) %>% mutate(focus=Proportion)
			}

			p <- ggplot(temp, aes(x=reorder(document, -focus), y=focus)) +
				geom_bar(stat="identity")

			p <- p + ggtitle(paste0("Distribution of '", word, "' across documents")) +
				theme_bw()

			if(type=="count"){
				p <- p + ylab("Count")
			}else{
				p <- p + ylab("Proportion")
			}

			if(!xaxis){
				p <- p + xlab("Documents") +
										theme(axis.text.x=element_blank(),
											axis.ticks.x=element_blank(),
											plot.title = element_text(hjust = 0.5))
			}else{
				p <- p + xlab("Documents")
				p <- p + theme(axis.text.x = element_text(angle = 90, hjust = 1),
											 plot.title = element_text(hjust = 0.5))
			}

			return(p)

		},

		visualize_words = function(words, n_show=1:num_uniquewords){
			# Distribution across words
				data %>%
					mutate(Show = if_else(.$Word %in% get("words"), "1", "0")) %>%
					slice(n_show) -> temp

				temp %>% 
					mutate(Width=if_else(Show==1, length(get("n_show"))/10, 1)) -> temp
				temp$right <- cumsum(temp$Width) + 30*c(0:(nrow(temp)-1))
				temp$left <- temp$right - temp$Width

				p <- ggplot(temp, aes(ymin = 0)) + 
					geom_rect(aes(xmin = left, xmax = right, ymax = `Proportion(%)`, colour = Show, fill = Show)) +
					scale_fill_manual("legend", values = c("0" = "#282828", "1" = "#d53800")) + 
					scale_colour_manual("legend", values = c("0" = "#282828", "1" = "#d53800")) + 
					xlab("Words") + ylab("Proportion (%)") + ggtitle("Distribution of words") +
					theme_bw() +
					theme(axis.text.x=element_blank(),
								axis.ticks.x=element_blank(),
								legend.position="none",
								plot.title = element_text(hjust = 0.5))

			return(p)

		},

		top_words = function(n_show=20){
			"Show frequent words"
				if(length(n_show)>1){
					data %>%
						slice(n_show) %>%
						arrange(-WordCount) %>%
						print(n=nrow(.))
				}else{
					data %>%
						top_n(n_show, WordCount) %>%
						arrange(-WordCount) %>%
						print(n=nrow(.))
				}

		},

		visualize_tfidf = function(seed_list){

			names(seed_list) <- paste0("EstTopic", 1:length(seed_list))
			seeds <- lapply(seed_list, function(x){unlist(strsplit(x," "))})
			ext_k <- length(seeds)
			max_num_words <- max(unlist(lapply(seeds, function(x){length(x)})))

			seeds_df <- data.frame(EstTopic=1, Word=1)
			for(k in 1:ext_k){
				words <- seeds[[k]]
				numwords <- length(words)
				topicname <- paste0("EstTopic", k)
				for(w in 1:numwords){
					seeds_df <- rbind(seeds_df, data.frame(EstTopic=topicname, Word=words[w]))
				}
			}
			seeds_df <- seeds_df[2:nrow(seeds_df), ]

			data_tfidf %>%
				inner_join(., seeds_df, by=c("term"="Word")) %>%
				ggplot(aes(x=tf_idf, y=..density.., colour=EstTopic)) +
					geom_density(stat = "density", position = "identity") +
					xlab("TF-IDF") + ylab("Density") +
					ggtitle("TF-IDF Desity of Words in Dictionary") +
					theme_bw() +
					theme(plot.title = element_text(hjust = 0.5)) -> p1

			data_tfidf %>%
				inner_join(., seeds_df, by=c("term"="Word")) %>%
				group_by(EstTopic, term) %>%
				summarize(Median = median(tf_idf)) %>%
				ungroup() %>%
				group_by(EstTopic) %>%
				arrange(desc(Median)) %>%
				mutate(Ranking = 1:n()) %>%
				ggplot(., aes(x=Ranking, y=Median, colour=EstTopic)) +
				geom_line() +
				geom_point() +
				geom_label_repel(aes(label = term), size=2.5,
												 box.padding = 0.20, label.padding = 0.12,
												 arrow=arrow(angle=10, length = unit(0.10, "inches"), ends = "last", type = "closed"),
												 show.legend = F) +
				scale_x_continuous(breaks=1:max_num_words) +
				ylab("Median TF-IDF") +
				ggtitle("Median TF-IDF of Words in Dictionary") +
				theme_bw() +
				theme(plot.title = element_text(hjust = 0.5)) -> p2

			return(list(density=p1, median=p2))

		},

		words_distribution = function(n_show=num_uniquewords){
			# Distribution across documents
				if(length(n_show)>1){
					data %>%
						slice(n_show) %>%
						arrange(-WordCount) -> temp
				}else{
					data %>%
						top_n(n_show, WordCount) %>%
						arrange(-WordCount) -> temp
				}

				p <- ggplot(temp, aes(x=Word, y=`Proportion(%)`)) +
					geom_bar(stat="identity") +
					scale_x_discrete(limits = temp$Word)

				p <- p +
					ggtitle(paste0("Distribution of words")) +
					ylab("Proportion (%)") +
					theme_bw()

				if(nrow(temp) <= 50){
					p <- p + theme(axis.text.x = element_text(angle = 90, hjust = 1),,
												 plot.title = element_text(hjust = 0.5))
				}else{
					p <- p + 
						theme(axis.text.x=element_blank(),
									axis.ticks.x=element_blank(),
									plot.title = element_text(hjust = 0.5))
				}
			

			return(p)

		}
	)
)


#' Explore Documents
#'
#' Explore the documents.
#'
#' @param files names of each file to read
#' @param encoding File encoding (Default: whatever \code{quanteda} guesses)
#' @param lowercase whether to transform each token to lowercase letters
#' @param remove_numbers whether to remove numbers
#' @param remove_punct whether to remove punctuation
#' @param remove_symbols whether to remove non-alphanumerical symbols
#' @param remove_separators whether to remove tabs, spaces, newlines, etc.
#' @param remove_twitter whether to remove Twitter detritus
#' @param remove_hyphens whether to split hyphenated words
#' @param remove_url whether to remove URLs
#' @param stem_language if not NULL, the language to use for stemming
#' @param stopwords if not NULL, a character vector of words to remove,
#'                  e.g. \code{quanteda::stopwords("english")}
#'
#' @return A R4 objects \describe{
#'         \item{show_words(word, type="count", n_show=100, xaxis=F)}{ show a word distribution. \code{type} should be \code{count} or \code{proportion}.}
#'         \item{top_words(n_show=10)}{ It selects the top n rows. If \code{n_show} is negative, it selects the bottom n rows.}
#'         }.
#' @importFrom quanteda corpus docvars tokens tokens_tolower tokens_remove tokens_wordstem dictionary dfm
#' @importFrom tidytext tidy
#' @import ggplot2
#' @import methods
#' @import dplyr
#' @export
explore <- function(files, encoding = "UTF-8",
                            lowercase = TRUE,
                            remove_numbers = TRUE, remove_punct = TRUE,
                            remove_symbols = TRUE, remove_separators = TRUE,
                            remove_twitter = FALSE, remove_hyphens = FALSE,
                            remove_url = TRUE, stem_language = NULL,
                            stopwords = NULL){

  args <- list(remove_numbers = remove_numbers,
               remove_punct = remove_punct,
               remove_symbols = remove_symbols,
               remove_separators = remove_separators,
               remove_twitter = remove_twitter,
               remove_hyphens = remove_hyphens,
               remove_url = remove_url)

  ## preprocess each text
  ## for debugging
  df <- data.frame(text = unlist(lapply(files, function(x){ paste0(readLines(x, encoding = encoding),
                                                                collapse = "\n") })),
                   stringsAsFactors = FALSE)
  df$doc_id <- paste0("text", 1:nrow(df))
  args$x <- corpus(df)
  ## debugging until here

  # args$x <- corpus(readtext(file_pattern, encoding = encoding))
  doc_names <- docvars(args$x, "doc_id") # docnames
  toks <- do.call(tokens, args = args)
  if (lowercase)
    toks <- tokens_tolower(toks)
  if (!is.null(stopwords))
    toks <- tokens_remove(toks, stopwords)
  if (!is.null(stem_language))
    toks <- tokens_wordstem(toks, language = stem_language)

	dfm_ <- dfm(toks)
	tidy_ <- tidy(dfm_)

	res <- ExploreDocuments$new(tidy_)

	return(res)

}
