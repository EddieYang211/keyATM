---
title: "Election Platforms 2: Why?"
author: "Will Lowe"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{election-platforms}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = ""
)

library(topicdict)
library(quanteda)
library(ca)
library(dplyr)
library(ggplot2)
theme_set(theme_minimal())
```

```{r}
dictfile <- system.file("extdata/laver-garry-ajps.ykd", package="topicdict")
# We'll use just the economics sections of this dictionary
dict <- dictionary(file = dictfile)[["Laver and Garry"]][["State in Economy"]]
dict$Neut <- NULL # not interested in this category! Just Pro / Con
# load the corpus
data("corpus_uk_platforms")
```

Preprocess the dictionary to expand the wildcards and filter out low frequency
terms
```{r}
post70s_corpus <- corpus_subset(corpus_uk_platforms, date > 1970)
partyabbrev <- c("Con", "Lib", "Lab", "LD", "LibSDP")
res_main_parties <- subset(res, ) # for plotting
docvars(post70s_corpus, "main") <- docvars(post70s_corpus, "party") %in% partyabbrev
new_dict <- preprocess_dictionary(dict, post70s_corpus, min.freq = 5,
                                  remove = stopwords(), remove_numbers = TRUE,
                                  remove_punct = TRUE, remove_symbols = TRUE,
                                  remove_separators = TRUE, remove_hyphens = FALSE)
cbind(old = lapply(dict, length),
      new = lapply(new_dict, length))
```


```{r}
dtm_orig <- dfm(post70s_corpus,
                dictionary = new_dict,
                remove = stopwords(), remove_numbers = TRUE,
                remove_punct = TRUE, remove_symbols = TRUE,
                remove_separators = TRUE, remove_hyphens = FALSE)
res <- data.frame(dtm_orig)
res <- data.frame(docvars(dtm_orig),
                     POS = (res$Con - res$Pro) / (res$Con + res$Pro),
                     logitPOS = log(res$Con) - log(res$Pro))
# fit a scaling model
dd <- dfm(post70s_corpus,
          remove = stopwords(), remove_numbers = TRUE,
          remove_punct = TRUE, remove_symbols = TRUE,
          remove_separators = TRUE, remove_hyphens = FALSE)
dd <- dfm_select(dd, unlist(new_dict))
dd <- dfm_trim(dd, min_count = 5, min_docfreq = 5)
mod <- ca(as.matrix(dd))
res$scaling1 <- -mod$rowcoord[,1] # fix an arbitrary sign flip
res$scaling2 <- mod$rowcoord[,2]  # this seems to fit the substantive story better

meta <- docvars(post70s_corpus) # use this when needed

```
One substantively sensible way to plot this 'gold standard' is as time series of
the main parties
```{r, fig.width=15, fig.height=10}
ggplot(res[meta$main,], aes(date, scaling1, color = party)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = c(1992, 1997), alpha = 0.1, size = 5 ) +
  scale_color_manual(values = c("blue", "red", "orange", "orange", "orange")) +
  coord_flip()
```

For reference, here's a straight word count scaling using only
the items in `new_dict`:
```{r, fig.width=15, fig.height=10}
ggplot(res[meta$main,], aes(date, scaling2, color = party)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = c(1992, 1997), alpha = 0.1, size = 5 ) +
  scale_color_manual(values = c("blue", "red", "orange", "orange", "orange")) +
  coord_flip()
```

We can generate 'wordscore' type quantities by looking at the logit of the word
generation probabilities from the topic model and comparing them to the scaling
scores:

```{r}
set.seed(1234)
mod5 <- topicdict_model(post70s_corpus, dict = new_dict,
                stopwords = stopwords(), remove_numbers = TRUE,
                remove_punct = TRUE, remove_symbols = TRUE,
                remove_separators = TRUE, remove_hyphens = FALSE, 
                extra_k = 5)
mod5 <- topicdict_train(mod5, 340)
post <- posterior(mod5)

on_topic <- (post$beta["Pro", new_dict$Pro]) 
off_topic <- (colSums(post$beta[-which(rownames(post$beta) == "Pro"), new_dict$Pro]))
vals_left <- sort(log(on_topic)- log(off_topic))
vals_left[is.infinite(vals_left) & vals_left > 0] <- 6 # Inf
vals_left[is.infinite(vals_left) & vals_left < 0] <- -6 # -Inf
cols <- ifelse(vals_left >= 6, "darkgreen", 
               ifelse(vals_left > 0, "green", 
                      ifelse(vals_left > -6, "red", "darkred")))

extremes_left <- which(abs(vals_left) == 6)
inbetweens_left <- which(abs(vals_left) != 6)
```

## Allegedly Left Vocab in the Full Model

```{r}
dotchart(vals_left[extremes_left], 
         labels = names(vals_left)[extremes_left], 
         color = cols[extremes_left], 
         pch = 19, main = "Decisively topical vocabulary (Left)")
```

```{r}
dotchart(vals_left[inbetweens_left], 
         labels = names(vals_left)[inbetweens_left], 
         color = cols[inbetweens_left], 
         pch = 19, main = "Borderline vocabulary (Left)")
```

## Allegedly Right Vocab in the Full Model

```{r}
on_topic <- (post$beta["Con", new_dict$Con]) 
off_topic <- (colSums(post$beta[-which(rownames(post$beta) == "Con"), new_dict$Con]))
vals_right <- sort(log(on_topic)- log(off_topic))
vals_right[is.infinite(vals_right) & vals_right > 0] <- 6 # Inf
vals_right[is.infinite(vals_right) & vals_right < 0] <- -6 # -Inf
cols <- ifelse(vals_right >= 6, "darkgreen", 
               ifelse(vals_right > 0, "green", 
                      ifelse(vals_right > -6, "red", "darkred")))

extremes_right <- which(abs(vals_right) == 6)
inbetweens_right <- which(abs(vals_right) != 6)
```


```{r}
dotchart(vals_right[extremes_right], 
         labels = names(vals_right)[extremes_right], 
         color = cols[extremes_right], 
         pch = 19, main = "Decisively topical vocabulary (Right)")
```

```{r}
dotchart(vals_right[inbetweens_right], 
         labels = names(vals_right)[inbetweens_right], 
         color = cols[inbetweens_right], 
         pch = 19, main = "Borderline vocabulary (Right)")
```

But if we give the model just a few of the strongly Left and Right words will it 
infer the rest?  Here, random samples of 10 in each category and the same
model specification (K + 5).

```{r}
sample_size <- 10
sample_dict <- dictionary(list(Pro=new_dict$Pro[extremes_left[sample(length(extremes_left), sample_size)]], Con = new_dict$Con[extremes_right[sample(length(extremes_right),   sample_size)]]))

# fit model 
set.seed(1234)

recovered <- data.frame(left = rep(NA, 20), right = rep(NA, 20))
for (i in 1:20){
sample_mod5 <- topicdict_model(post70s_corpus, dict = sample_dict,
                stopwords = stopwords(), remove_numbers = TRUE,
                remove_punct = TRUE, remove_symbols = TRUE,
                remove_separators = TRUE, remove_hyphens = FALSE, 
                extra_k = 5)
sample_mod5 <- topicdict_train(sample_mod5, 340)
sample_post <- posterior(sample_mod5)

# Left
other_vocab_left <- setdiff(new_dict$Pro, sample_dict$Pro)
on_topic <- sample_post$beta["Pro", other_vocab_left] 
off_topic <- colSums(sample_post$beta[rownames(sample_post$beta) != "Pro", 
                               other_vocab_left])
vals_left <- sort(log(on_topic)- log(off_topic))
vals_left[is.infinite(vals_left) & vals_left > 0] <- 6 # Inf
vals_left[is.infinite(vals_left) & vals_left < 0] <- -6 # -Inf

recovered$left[i] <- mean(vals_left > 0)

# Right
other_vocab_right <- setdiff(new_dict$Con, sample_dict$Con)
on_topic <- sample_post$beta["Con", other_vocab_right] 
off_topic <- colSums(sample_post$beta[rownames(sample_post$beta) != "Con", 
                               other_vocab_right])

vals_left <- sort(log(on_topic)- log(off_topic))
vals_right[is.infinite(vals_right) & vals_right > 0] <- 6 # Inf
vals_right[is.infinite(vals_right) & vals_right < 0] <- -6 # -Inf

recovered$right[i] <- mean(vals_right > 0)
}
```



