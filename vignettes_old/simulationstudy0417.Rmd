---
title: "Progress Report"
author: "Shusei Eshima"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{GenSimDataLDA}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r warning=FALSE, message=FALSE, echo = FALSE}
# package_folder <- "/Users/Shusei/Dropbox/Study/Project/ImaiText/topicdict"
# setwd(package_folder)
# devtools::document() ; devtools::install() ; library(topicdict)
savepath <- "/Users/Shusei/Dropbox/Study/Project/ImaiText/topicdict/vignettes/"
library(knitr)
knitr::opts_chunk$set(comment = "")
```


```{r, warning=FALSE, message=FALSE}
library(topicdict)
library(quanteda)
library(tibble)
library(ggplot2)
library(dplyr)
library(topicmodels)
```

# Read Data
```{r, warning=FALSE, message=FALSE}
iter_num <- 50
folder <- paste0("/Users/Shusei/Dropbox/Study/My_Research/TreeStructuredTopicModel/Papers/replication/Catalinac/data/docs") # Data from original data Document-Term Matrix
docs <- list.files(folder, pattern = "*.txt", full.names = TRUE)
explore_ <- explore(docs,
             remove_numbers = TRUE,
             remove_punct = TRUE,
             remove_symbols = TRUE,
             remove_separators = TRUE)
```

Create model function
```{r, warning=FALSE, message=FALSE}
create_model <- function(docs, seed_list, extra_k){
	set.seed(225)
	names(seed_list) <- 1:length(seed_list)
	dict <- quanteda::dictionary(seed_list)
  model <- topicdict_model(docs,
               dict = dict, extra_k = extra_k,
               remove_numbers = TRUE, 
               remove_punct = TRUE,
               remove_symbols = TRUE,
               remove_separators = TRUE)

  return(model)
}
```

# About the original paper
##  Selection of the number of topics
"After experimenting with specifications of between 20 and 200 topics, we settled on a speciﬁcation of 69 topics because it was one of the lowest specifications that appeared to produce topics that were ﬁne-grained enough to resemble our three quantities of interest." (Appendix B, p.2)

## Topics the author uses to validate the result in the Appendix  
### Party-platform topics
Topics that are primarily discussed by the candidates of a single party in a single election (Appendix C, p.3)

### Issue topics
Topics that are primarily discussed by candidates of more than one party in more than one election

## List of topics explained in Appendix
### Agriculture, Forestry, and Fisheries (Topic 52 in the original output)
Top words: agriculture, industry, manufacture, provide, carry out, fishing, akita (the name of prefecture), develop, push for, prefecture, merchant, yamagata (the name of prefecture), community, development, technology 

### Worry About Earthquakes and Nuclear Power (Topic 62 in the original output)
Top words: fukui (the name of prefecture), shizuoka (the name of prefecture), recovery, coalition, kobe (the name of prefecture), gifu (the name of prefecture), economy, ruling party, prefecture, disaster victim, airport, industry, government, disaster, person

### Liberal Democracy is the Best (Topic 63 in the original output) 
Top words: politic, ism, liberal, society, democracy, osaka (the name of prefecture), peace, realize, japan, human, aim for, rich, welfare, protect, person

### Consumption Tax is to Fund the Military (Topic 20 in the original output)
Top words: tax, politic, consumption, abolish, ldp, citizen, liberal, japan, protect, rice, business, import, realize, donation, block

### Benefits for Organized Groups (Topic 58 in the original paper)
Top words: business, education, small-medium, enrich, carry out, welfare, push for, better, stable, household, stimulate, measure,benefits, building, pension

## Particularistic topics and programmatic topics
The author reads manifestos (documents) with high probabilities of belonging to each topic and collects the groups of people candidates suggested would benefit from the promises associated with it. Next, the author calculates the proportion of each beneficiary in Japan's population using official sources. 

The author defines policies that purport to benefit large groups of voters as "programmatic" and policies that purport to benefit small groups of voters as "particularistic". Calculation of the percentage of Japan's population each topic purports to benefit allows the author to use topic's largest beneficiary to classify topics into particularistic and programmatic.  

## Seeds List
```{r, warning=FALSE, message=FALSE}
# Top fifteen words appeared in the original output
topic52 <- c("農業 産業 工業 整備 図る 漁業 秋田 開発 進める 県 商 山形 地域 発展 技術")
topic62 <- c("福井 静岡 復興 連立 神戸 岐阜 経済 与党 県 被災 空港 産業 政権 災害 ひと")
topic63 <- c("政治 主義 自由 社会 民主 大阪 平和 実現 日本 人間 目指す 豊か 福祉 守る ひと")
topic20 <- c("税 政治 消費 廃止 自民党 国民 自由 日本 守る コメ 企業 輸入 実現 献金 阻止")
topic58 <- c("企業 教育 中小 充実 図る 福祉 進める 改善 安定 家庭 振興 対策 恩給 作り 年金")

seed_list <- list(topic52, topic62, topic63, topic20, topic58)
```

# Explore Data
```{r, warning=FALSE, message=FALSE, fig.width=9.5, fig.height=6.5}
# Use a part of seed words
use_part_of_seed <- function(x, use=1:5){
	words <- strsplit(x, " ")[[1]][use] # use part of seed
	res <- paste(words, collapse=" ") # back to seed_list format
	return(res)
}


# Top words
explore_$top_words(n=10)
	# politics, japan, society, reform, people, party, realize, tax, education, defend

# Proportion
p <- explore_$visualize_dict_prop(seed_list)
ggsave(paste0(savepath, "Xseeds_all.pdf"), p, family="Japan1GothicBBB")

p <- explore_$visualize_dict_prop(lapply(seed_list, use_part_of_seed, use=1:8))
ggsave(paste0(savepath, "Xseeds_1-8.pdf"), p, family="Japan1GothicBBB")
```

# Fit Seeded LDA
## K+1 rule with different number of keywords 
```{r, warning=FALSE, message=FALSE}
model <- create_model(docs, seed_list, extra_k=1)
res <- topicdict_train(model, iter = iter_num)
post <- topicdict::posterior(res)
```


We'll use the Bara data, and seeds extracted from the dictionary.  
These are all in package `extdata`:
```{r, warning=FALSE, message=FALSE}
doc_folder <- system.file(file.path("extdata", "bara_paras"), package = "topicdict")
seed_file <- system.file(file.path("extdata", "bara_seeds.txt"), package = "topicdict")

## turn this flat file of seeds into a (quanteda dictionary) list as it would be used
seed_list <- Map(function(x){ unlist(strsplit(x, " ")) }, 
                 Filter(function(x){ !grepl(x, pattern = "#") }, # remove comments
                        readLines(seed_file)))

# Original topic names from Bara dictionary
names(seed_list) <- c("advocacy", "legal", "medical", "moral", "procedural", "social")
dict <- dictionary(seed_list)
dict
```

Initialize a model with the default tokenization options, without removing
stopwords or stemming, and run the model.
```{r, warning=FALSE, message=FALSE}
set.seed(225)

stops <- setdiff(c(stopwords("english"), letters), 
                  c("he", "his", "him", "himself", "she", "hers", "her", "herself"))
model <- topicdict_model(list.files(doc_folder, pattern="txt", full.names=T), extra_k=extra_k,
                         dict, stopwords = stops)
res <- topicdict_train(model, iter = 500)
original_vocabulary <- model$vocab
```

# Create Data 
## Organize fitted data
```{r, warning=FALSE, message=FALSE}
collapse <- function(obj){
  temp <- unlist(obj) 
  names(temp) <- NULL
  return(temp)
}
W <- collapse(res$W)
Z <- collapse(res$Z)
X <- collapse(res$X)
fit <- tibble(W=W, Z=Z, X=X)


# phiR
fit %>%
  filter(X == 0) %>%
  group_by(W, Z) %>%
  summarize(count_wordtopic = n()) %>%
  ungroup() %>%
  group_by(Z) %>%
  mutate(count_topicsum = sum(count_wordtopic)) %>%
  ungroup() %>%
  mutate(phi_r = count_wordtopic / count_topicsum) %>%
  select(W, Z, phi_r) %>%
  tidyr::spread(W, phi_r, fill=1e-12) %>%
  select(-Z) -> fitted

phiR <- t(apply(fitted%>%as.matrix(), 1, function(x)( return(x / sum(x)) )))

# phiS
fit %>%
  filter(X == 1) %>%
  group_by(W, Z) %>%
  summarize(count_wordtopic = n()) %>%
  ungroup() %>%
  group_by(Z) %>%
  mutate(count_topicsum = sum(count_wordtopic)) %>%
  ungroup() %>%
  mutate(phi_s = count_wordtopic / count_topicsum) %>%
  select(W, Z, phi_s) %>%
  tidyr::spread(W, phi_s, fill=1e-12) %>%
  select(-Z) -> fitted

phiS <- t(apply(fitted%>%as.matrix(), 1, function(x)( return(x / sum(x)) )))


# p
fit %>% 
  group_by(X, Z) %>%
  summarize(count_xtopic = n()) %>%
  ungroup() %>%
  group_by(Z) %>%
  mutate(count_topicsum = sum(count_xtopic)) %>%
  ungroup() %>%
  mutate(p = count_xtopic / count_topicsum) %>%
  filter(X==1) %>%
  select(p) %>% as.matrix() %>% as.vector() -> p_original_vec

# alpha
alpha_vec <- res$alpha
```

## Preparation for creating simulation data 
```{r, warning=FALSE, message=FALSE}
rbern <- function(n, prob){
  return(rbinom(n, 1, prob))
}
rcat <- function(n, p){
  if (is.vector(p)) {
    x <- as.vector(which(rmultinom(n, size=1, prob=p) == 1, arr.ind=TRUE)[, "row"])
  } else {
    d <- dim(p)
    n <- d[1]
    k <- d[2]
    lev <- dimnames(p)[[2]]
    if (!length(lev)) lev <- 1:k
    z <- colSums(p)
    U <- apply(p, 1, cumsum)
    U[,k] <- 1
    un <- rep(runif(n), rep(k,n))
    x <- lev[1 + colSums(un > U)]}
    return(x)
}
## From MCMCpack
rdirichlet <- function(n, alpha) {
  l <- length(alpha)
  x <- matrix(rgamma(l*n,alpha), ncol=l, byrow=TRUE)
  sm <- x %*% rep(1,l)
  return(x / as.vector(sm))
}

CreateDir_wzx <- function(saveDir){
  dir1 <- paste0(saveDir, "W")
  dir.create(dir1)
  dir2 <- paste0(saveDir, "Z")
  dir.create(dir2)
  dir3 <- paste0(saveDir, "X")
  dir.create(dir3)
  return(list(dir1, dir2, dir3))
}

Gen_ND <- function(document, lambda){
  nd <- numeric(document)
  ## generate document length
  nd <- rpois(document, lambda)
  return(nd)
}

Gen_theta <- function(doc_len, alpha_vec){
  theta <- rdirichlet(doc_len, alpha_vec)
  return(theta)
}

Gen_z <- function(theta, d, doc_len){
  z <- rcat(doc_len, theta[d,])
  return(z)
}

Gen_x <- function(topic_num, probX){
  x <- rbern(1, probX[topic_num])
  return(x)
}

Gen_w <- function(phi, topic){
  tmp <- rcat(1, phi[topic,])
  words <- colnames(phi)
  return(words[tmp])
}

Gen_w_seeds <- function(index, phiR, phiS, z, x){
  topic <- z[index]
  indicator <- x[index]

  if (indicator == 0){
    # Regular words
    word <- Gen_w(phiR, topic)
    # word <- original_vocabulary[as.numeric(word)] # original_vocabulary is extracted from the trained model outside
    word <- paste0("W", as.character(word), "t", as.character(topic))
  } else {
    # Seed words
    word <- Gen_w(phiS, topic)
    # word <- original_vocabulary[as.numeric(word)]
    word <- paste0("W", as.character(word), "t", as.character(topic))
  }
  return(word)
}

Gen_wzx <- function(doc_id, doc_len, phiR, phiS, p_vec, theta){
  z <- Gen_z(theta, doc_id, doc_len)
  x <- sapply(z, Gen_x, probX=p_vec)
  w <- sapply(1:doc_len, Gen_w_seeds, phiR=phiR, phiS=phiS, z=z, x=x)
  # print(w)
  return(list(doc_id=rep(doc_id, doc_len), w=w,z=z,x=x))
}

Write_wzx <- function(content, name, saveDir){
  txt <- paste(as.character(content), collapse = " ")
  name <- paste0(saveDir, name, ".txt")
  write(txt, name)
}

Save_wzx <- function(data, i, saveDir){
  ## save W for document i

  Write_wzx(data[["w"]], paste0("text", i), paste0(saveDir,"W/"))

  ## save Z for document i
  Write_wzx(data[["z"]], paste0("z_", i), paste0(saveDir,"Z/"))

  ## save X for document i
  Write_wzx(data[["x"]], paste0("x_", i), paste0(saveDir,"X/"))
}

create_sim_data_from_fitted <- function(
  saveDir, alpha, phiR, phiS, p,
  D=200, lambda=300, rand_seed=123
){
  # Set Seed
  set.seed(rand_seed)

  # Prepare Directory
  dir.create(saveDir)
  Dirs <- CreateDir_wzx(saveDir)

  # Set the number of topics
  K <- nrow(phiS)
  phiR <- phiR[1:K, ]
  alpha_vec <- alpha[1:K]
  V <- ncol(phiR)
  p_vec <- p[1:K]

  # Length of the documents
  nd <- Gen_ND(D, lambda)

  # Get theta
  theta <- Gen_theta(D, alpha_vec)

  ## Generate topic and word for each word in each document
  doc_list <- vector("list", D)
  
  for (i in 1:D){
    # the length of the document i
    tmp_nd <- nd[i]
  
    # ## a list of vector to contain the results
    # tmp <- WZX_Vec(tmp_nd)
  
    ## Generate W, Z, and X
    wzx_list <- Gen_wzx(i, tmp_nd, phiR, phiS, p_vec, theta)
  
    ## save W, Z, X for document i
    Save_wzx(wzx_list, i, saveDir)
  
    ## Store Created Docs
    doc_list[[i]] <- wzx_list
  
  }
}

show_true_seeds <- function(phiS){
  n <- nrow(phiS)
  seeds_ <- c()
  for(i in 1:n){
    temp <- phiS[i, ]
    seeds_ <- c(seeds_, names(temp[temp > 1e-8]))
  }
  return(seeds_)
}

create_model <- function(data_folder_name, extra_k, 
                                     seed_file_name="", seed_list = NULL,
                                     cull_seed=NULL){

  # cull_seed: Which row (=topic) and column (=number of seed) to use

  set.seed(225)

  folder <- paste0(data_folder, data_folder_name, "/W")
  docs <- list.files(folder, pattern = "*.txt", full.names = TRUE)

  if(seed_file_name != ""){
    seed_file <- paste0(data_folder, data_folder_name, "/", seed_file_name)
    seeddata <- invisible(readr::read_csv(seed_file))
  }

  if(!is.null(cull_seed)){
    seeddata <- seeddata[1:cull_seed[1], 1:cull_seed[2]]
  }

  if(is.null(seed_list)){
    seed_list <- as.list( apply(seeddata, 1, function(x){return(paste0(x, collapse=" "))}) )
  }
  names(seed_list) <- 1:length(seed_list)
  dict <- quanteda::dictionary(seed_list)

  model <- topicdict_model(docs,
               dict = dict, extra_k = extra_k,
               remove_numbers = FALSE, # For simulation, make it false
               remove_punct = TRUE,
               remove_symbols = TRUE,
               remove_separators = TRUE)

  return(model)
}

diagnosis_topic_recovery_heatmap <- function(post, n=25, 
                        topicvec=c(), merge=list()){
  topwords <- top_terms(post, n=n)
  topwords <- data.frame(topwords)
  colnames(topwords) <- paste0("EstTopic", 1:ncol(topwords))

  topwords <- tidyr::gather(topwords, key=EstTopic, value=Word)

  topwords %>%
    mutate(RawWord = Word) %>%
    tidyr::separate(Word,
        into=c("word_id", "TrueTopic"),
        sep="t") %>%
    mutate(TrueTopic = paste0("True", as.character(TrueTopic))) -> res_

  merge_length <- length(merge)
  if(merge_length != 0){
    # Merge Topics
    for(i in 1:merge_length){
      m <- merge[[i]]
      mt <- paste0("True", m)

      res_ %>%
        mutate(TrueTopic=replace(TrueTopic, TrueTopic==mt[1], mt[3])) %>%
        mutate(TrueTopic=replace(TrueTopic, TrueTopic==mt[2], mt[3])) -> res_
    }
  }

  res_ %>%
    group_by(EstTopic, TrueTopic) %>%
    summarise(counts = n()) %>%
    ungroup() %>%
    group_by(EstTopic) %>% 
    mutate(topicsum = sum(counts)) %>%
    ungroup() %>%
    mutate(Proportion = counts / topicsum * 100) -> res_

  num <- length(unique(res_$EstTopic))
  if(is.null(topicvec)){
		res_ %>%
			group_by(EstTopic) %>%
			top_n(1, Proportion) %>%
			arrange(TrueTopic) %>%
			select(EstTopic) -> topicvec 
		topicvec <-	unique(as.integer(gsub("EstTopic", "", topicvec$EstTopic)))
  }else if(length(topicvec) != num){
		message("topicvec length does not match")
		topicvec <- 1:num
  }

  truenum <- length(unique(res_$TrueTopic))

  title <- paste0("Seeded LDA: Top ", as.character(n), " words")

  g <- ggplot(res_, aes(EstTopic, TrueTopic)) +
        geom_tile(aes(fill=Proportion)) + 
        scale_fill_gradient(limits=c(0, 100), low="#e8e8e8", high="#0072B2", name = "Proportion") +
        scale_x_discrete(limits = rev(paste0("EstTopic", topicvec))) +
        coord_flip() +
        # scale_y_discrete(limits = paste0("True", 1:truenum)) +
        xlab("Estimated Topics") + ylab("True Topic") + theme_bw(base_size=13) +
        ggtitle(title) +
        theme(plot.title = element_text(hjust = 0.5))

  return(g)
}

compare_seededLDA <- function(seed_list, total_k, folder_name="Sim1"){
	extra_k <- total_k - length(seed_list)
	model <- create_model(folder_name, seed_list=seed_list, extra_k=extra_k)
	res <- topicdict_train(model, iter = iter_num)
	post <- topicdict::posterior(res)
	diagnosis_topic_recovery_heatmap(post, 25)
}
```

## LDA
```{r, warning=FALSE, message=FALSE}
library(tm)

get_lda_result <- function(data_folder_name, iter, k, topicvec=1:k, show_n=25){
  folder <- paste0(data_folder, data_folder_name, "/W")

  # Prepare Data
  corpus <- Corpus(DirSource(folder))
  strsplit_space_tokenizer <- function(x)
      unlist(strsplit(as.character(x), "[[:space:]]+"))

  dtm <- DocumentTermMatrix(corpus,
                           control = list(tokenize=strsplit_space_tokenizer, 
                           stopwords = F, tolower = F, 
                           stemming = F, wordLengths = c(1, Inf)))

  lda <- LDA(dtm, k = k, control = list(seed = 225, iter=iter), method="Gibbs")

  assign <-tidytext::augment(lda, dtm)
  assign %>% 
        mutate(Word=term) %>%
        tidyr::separate(term,
            into=c("word_id", "TrueTopic"),
            sep="t") %>%
        mutate(TrueTopic = paste0("True", as.character(TrueTopic))) %>%
        group_by(.topic, TrueTopic, Word) %>%
        summarise(counts = sum(count)) %>%
        ungroup() %>%
        group_by(.topic) %>%
        top_n(show_n, counts) %>%
        ungroup() %>%
        group_by(.topic, TrueTopic) %>%
        summarise(counts = sum(counts)) %>%
        ungroup() %>%
        group_by(.topic) %>%
        mutate(topicsum = sum(counts)) %>%
        ungroup() %>%
        mutate(Proportion = counts / topicsum * 100,
               EstTopic = paste0("EstTopic", .topic)) -> res

	res %>%
		group_by(EstTopic) %>%
		top_n(1, Proportion) %>%
		arrange(TrueTopic) %>%
		select(EstTopic) -> topicvec 
	topicvec <-	unique(as.integer(gsub("EstTopic", "", topicvec$EstTopic)))

  num <- length(unique(res$EstTopic))
  truenum <- length(unique(res$TrueTopic))
  title <- paste0("LDA: Top ", as.character(show_n), " words")

  g <- ggplot(res, aes(EstTopic, TrueTopic)) +
        geom_tile(aes(fill=Proportion)) + 
        scale_fill_gradient(limits=c(0, 100), low="#e8e8e8", high="#0072B2") +
        scale_x_discrete(limits = rev(paste0("EstTopic", topicvec))) +
        coord_flip() +
        scale_y_discrete(limits = paste0("True", 1:truenum)) +
        xlab("Estimated Topics") + ylab("True Topic") + theme_bw(base_size=13) +
        ggtitle(title) +
        theme(plot.title = element_text(hjust = 0.5))

  return(g)
}
```


# Generate Data
```{r, warning=FALSE, message=FALSE}
data_folder <- tempfile()
name <- "Sim1"
create_sim_data_from_fitted(saveDir=paste0(data_folder, name, "/"), alpha_vec, phiR, phiS, p_original_vec, D=1000, lambda=200)
```

# At least one strong keyword
```{r, warning=FALSE, message=FALSE, fig.width=7, fig.height=5}
# Explore
folder <- paste0(data_folder, "Sim1", "/W")
docs <- list.files(folder, pattern = "*.txt", full.names = TRUE)
explore_ <- explore(docs,
             remove_numbers = FALSE, # For simulation, make it false
             remove_punct = TRUE,
             remove_symbols = TRUE,
             remove_separators = TRUE)
explore_$top_words()
```

## Type 1
```{r, warning=FALSE, message=FALSE, fig.width=9.5, fig.height=6.5}
seed_list <- list(c("w67t1 w181t1 w104t1"), #0.695
                 c("w0t2 w999t2 w554t2"), # 0.481 
                 c("w127t3 w245t3 w1043t3")) # 0.336
explore_$visualize_dict_prop(seed_list)
compare_seededLDA(seed_list, total_k=8)
```


## Type 2
```{r, warning=FALSE, message=FALSE, fig.width=9.5, fig.height=6.5}
seed_list <- list(c("w118t5 w181t1 w104t1"), #0.695
                 c("w0t2 w999t2 w554t2"), # 0.481 
                 c("w127t3 w245t3 w1043t3")) # 0.336
explore_$visualize_dict_prop(seed_list)
compare_seededLDA(seed_list, total_k=8)
```

## Type 3
```{r, warning=FALSE, message=FALSE, fig.width=9.5, fig.height=6.5}
seed_list <- list(c("w64t1 w181t1 w104t1"), #0.695
                 c("w0t2 w999t2 w554t2"), # 0.481 
                 c("w127t3 w245t3 w1043t3")) # 0.336
explore_$visualize_dict_prop(seed_list)
compare_seededLDA(seed_list, total_k=8)
```

## Type 4
```{r, warning=FALSE, message=FALSE, fig.width=9.5, fig.height=6.5}
seed_list <- list(c("w64t1 w181t1 w104t1"), #0.695
                 c("w223t2 w999t2 w554t2"), # 0.481 
                 c("w127t3 w245t3 w1043t3")) # 0.336
explore_$visualize_dict_prop(seed_list)
compare_seededLDA(seed_list, total_k=8)
```

# Change the number of dicrtionary words
`explore_$top_words(55:80)`

## Type 1
```{r, warning=FALSE, message=FALSE, fig.width=9.5, fig.height=6.5}
seed_list <- list(c("w83t1"), #0.695
                 c("w541t2 w536t2"), # 0.481 
                 c("w127t3 w77t3 w115t3")) # 0.336
explore_$visualize_dict_prop(seed_list)
compare_seededLDA(seed_list, total_k=8)
```

## Type 2
```{r, warning=FALSE, message=FALSE, fig.width=9.5, fig.height=6.5}
seed_list <- list(c("w449t1"), #0.695
                 c("w593t2 w536t2"), # 0.481 
                 c("w143t3 w77t3 w115t3")) # 0.336
explore_$visualize_dict_prop(seed_list)
compare_seededLDA(seed_list, total_k=8)
```


## Type 3
```{r, warning=FALSE, message=FALSE, fig.width=9.5, fig.height=6.5}
seed_list <- list(c("w449t1 w768t1"), #0.695
                 c("w593t2 w536t2"), # 0.481 
                 c("w143t3 w77t3 w115t3")) # 0.336
explore_$visualize_dict_prop(seed_list)
compare_seededLDA(seed_list, total_k=8)
```

## Type 3.5
```{r, warning=FALSE, message=FALSE, fig.width=9.5, fig.height=6.5}
seed_list <- list(c("w449t1 w768t1"), #0.695
                 c("w536t2"), # 0.481 
                 c("w143t3 w77t3 w115t3")) # 0.336
explore_$visualize_dict_prop(seed_list)
compare_seededLDA(seed_list, total_k=8)
```

## Type 3.7
```{r, warning=FALSE, message=FALSE, fig.width=9.5, fig.height=6.5}
seed_list <- list(c("w449t1 w768t1"), #0.695
                 c("w593t2 w536t2"), # 0.481 
                 c("w77t3")) # 0.336
explore_$visualize_dict_prop(seed_list)
compare_seededLDA(seed_list, total_k=8)
```

## Type 4
```{r, warning=FALSE, message=FALSE, fig.width=9.5, fig.height=6.5}
seed_list <- list(c("w449t1 w768t1 w106t1 w170t1 w20t1"), #0.695
                 c("w593t2 w536t2"), # 0.481 
                 c("w143t3 w77t3 w115t3")) # 0.336
explore_$visualize_dict_prop(seed_list)
compare_seededLDA(seed_list, total_k=8)
```

## Type 5
```{r, warning=FALSE, message=FALSE, fig.width=9.5, fig.height=6.5}
seed_list <- list(c("w449t1 w768t1 w106t1 w170t1 w20t1 w743t1 w86t1 w73t1"), #0.695
                 c("w593t2 w536t2"), # 0.481 
                 c("w143t3 w77t3 w115t3")) # 0.336
explore_$visualize_dict_prop(seed_list)
compare_seededLDA(seed_list, total_k=8)
```
