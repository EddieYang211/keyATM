---
title: "TF-IDF"
author: "Shusei Eshima"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{GenSimDataLDA}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r warning=FALSE, message=FALSE, echo = FALSE}
# package_folder <- "/Users/Shusei/Dropbox/Study/Project/ImaiText/topicdict"
# setwd(package_folder)
# devtools::document() ; devtools::install() ; library(topicdict)
library(knitr)
knitr::opts_chunk$set(comment = "")
```


```{r, warning=FALSE, message=FALSE}
library(topicdict)
library(quanteda)
library(tibble)
library(ggplot2)
library(dplyr)
library(topicmodels)
iter_num <- 350
extra_k <- 2
```

# Fit Seeded LDA
We'll use the Bara data, and seeds extracted from the dictionary.  
These are all in package `extdata`:
```{r, warning=FALSE, message=FALSE}
doc_folder <- system.file(file.path("extdata", "bara_paras"), package = "topicdict")
seed_file <- system.file(file.path("extdata", "bara_seeds.txt"), package = "topicdict")

## turn this flat file of seeds into a (quanteda dictionary) list as it would be used
seed_list <- Map(function(x){ unlist(strsplit(x, " ")) }, 
                 Filter(function(x){ !grepl(x, pattern = "#") }, # remove comments
                        readLines(seed_file)))

# Original topic names from Bara dictionary
names(seed_list) <- c("advocacy", "legal", "medical", "moral", "procedural", "social")
dict <- dictionary(seed_list)
dict
```

Initialize a model with the default tokenization options, without removing
stopwords or stemming, and run the model.
```{r, warning=FALSE, message=FALSE}
set.seed(225)

stops <- setdiff(c(stopwords("english"), letters), 
                  c("he", "his", "him", "himself", "she", "hers", "her", "herself"))
model <- topicdict_model(list.files(doc_folder, pattern="txt", full.names=T), extra_k=extra_k,
                         dict, stopwords = stops)
res <- topicdict_train(model, iter = 500)
original_vocabulary <- model$vocab
```

# Create Data 
## Organize fitted data
```{r, warning=FALSE, message=FALSE}
collapse <- function(obj){
  temp <- unlist(obj) 
  names(temp) <- NULL
  return(temp)
}
W <- collapse(res$W)
Z <- collapse(res$Z)
X <- collapse(res$X)
fit <- tibble(W=W, Z=Z, X=X)


# phiR
fit %>%
  filter(X == 0) %>%
  group_by(W, Z) %>%
  summarize(count_wordtopic = n()) %>%
  ungroup() %>%
  group_by(Z) %>%
  mutate(count_topicsum = sum(count_wordtopic)) %>%
  ungroup() %>%
  mutate(phi_r = count_wordtopic / count_topicsum) %>%
  select(W, Z, phi_r) %>%
  tidyr::spread(W, phi_r, fill=1e-12) %>%
  select(-Z) -> fitted

phiR <- t(apply(fitted%>%as.matrix(), 1, function(x)( return(x / sum(x)) )))

# phiS
fit %>%
  filter(X == 1) %>%
  group_by(W, Z) %>%
  summarize(count_wordtopic = n()) %>%
  ungroup() %>%
  group_by(Z) %>%
  mutate(count_topicsum = sum(count_wordtopic)) %>%
  ungroup() %>%
  mutate(phi_s = count_wordtopic / count_topicsum) %>%
  select(W, Z, phi_s) %>%
  tidyr::spread(W, phi_s, fill=1e-12) %>%
  select(-Z) -> fitted

phiS <- t(apply(fitted%>%as.matrix(), 1, function(x)( return(x / sum(x)) )))


# p
fit %>% 
  group_by(X, Z) %>%
  summarize(count_xtopic = n()) %>%
  ungroup() %>%
  group_by(Z) %>%
  mutate(count_topicsum = sum(count_xtopic)) %>%
  ungroup() %>%
  mutate(p = count_xtopic / count_topicsum) %>%
  filter(X==1) %>%
  select(p) %>% as.matrix() %>% as.vector() -> p_original_vec

# alpha
alpha_vec <- res$alpha
```

## Preparation for creating simulation data 
```{r, warning=FALSE, message=FALSE}
rbern <- function(n, prob){
  return(rbinom(n, 1, prob))
}
rcat <- function(n, p){
  if (is.vector(p)) {
    x <- as.vector(which(rmultinom(n, size=1, prob=p) == 1, arr.ind=TRUE)[, "row"])
  } else {
    d <- dim(p)
    n <- d[1]
    k <- d[2]
    lev <- dimnames(p)[[2]]
    if (!length(lev)) lev <- 1:k
    z <- colSums(p)
    U <- apply(p, 1, cumsum)
    U[,k] <- 1
    un <- rep(runif(n), rep(k,n))
    x <- lev[1 + colSums(un > U)]}
    return(x)
}
## From MCMCpack
rdirichlet <- function(n, alpha) {
  l <- length(alpha)
  x <- matrix(rgamma(l*n,alpha), ncol=l, byrow=TRUE)
  sm <- x %*% rep(1,l)
  return(x / as.vector(sm))
}

CreateDir_wzx <- function(saveDir){
  dir1 <- paste0(saveDir, "W")
  dir.create(dir1)
  dir2 <- paste0(saveDir, "Z")
  dir.create(dir2)
  dir3 <- paste0(saveDir, "X")
  dir.create(dir3)
  return(list(dir1, dir2, dir3))
}

Gen_ND <- function(document, lambda){
  nd <- numeric(document)
  ## generate document length
  nd <- rpois(document, lambda)
  return(nd)
}

Gen_theta <- function(doc_len, alpha_vec){
  theta <- rdirichlet(doc_len, alpha_vec)
  return(theta)
}

Gen_z <- function(theta, d, doc_len){
  z <- rcat(doc_len, theta[d,])
  return(z)
}

Gen_x <- function(topic_num, probX){
  x <- rbern(1, probX[topic_num])
  return(x)
}

Gen_w <- function(phi, topic){
  tmp <- rcat(1, phi[topic,])
  words <- colnames(phi)
  return(words[tmp])
}

Gen_w_seeds <- function(index, phiR, phiS, z, x){
  topic <- z[index]
  indicator <- x[index]

  if (indicator == 0){
    # Regular words
    word <- Gen_w(phiR, topic)
    # word <- original_vocabulary[as.numeric(word)] # original_vocabulary is extracted from the trained model outside
    word <- paste0("W", as.character(word), "t", as.character(topic))
  } else {
    # Seed words
    word <- Gen_w(phiS, topic)
    # word <- original_vocabulary[as.numeric(word)]
    word <- paste0("W", as.character(word), "t", as.character(topic))
  }
  return(word)
}

Gen_wzx <- function(doc_id, doc_len, phiR, phiS, p_vec, theta){
  z <- Gen_z(theta, doc_id, doc_len)
  x <- sapply(z, Gen_x, probX=p_vec)
  w <- sapply(1:doc_len, Gen_w_seeds, phiR=phiR, phiS=phiS, z=z, x=x)
  # print(w)
  return(list(doc_id=rep(doc_id, doc_len), w=w,z=z,x=x))
}

Write_wzx <- function(content, name, saveDir){
  txt <- paste(as.character(content), collapse = " ")
  name <- paste0(saveDir, name, ".txt")
  write(txt, name)
}

Save_wzx <- function(data, i, saveDir){
  ## save W for document i

  Write_wzx(data[["w"]], paste0("text", i), paste0(saveDir,"W/"))

  ## save Z for document i
  Write_wzx(data[["z"]], paste0("z_", i), paste0(saveDir,"Z/"))

  ## save X for document i
  Write_wzx(data[["x"]], paste0("x_", i), paste0(saveDir,"X/"))
}

create_sim_data_from_fitted <- function(
  saveDir, alpha, phiR, phiS, p,
  D=200, lambda=300, rand_seed=123
){
  # Set Seed
  set.seed(rand_seed)

  # Prepare Directory
  dir.create(saveDir)
  Dirs <- CreateDir_wzx(saveDir)

  # Set the number of topics
  K <- nrow(phiS)
  phiR <- phiR[1:K, ]
  alpha_vec <- alpha[1:K]
  V <- ncol(phiR)
  p_vec <- p[1:K]

  # Length of the documents
  nd <- Gen_ND(D, lambda)

  # Get theta
  theta <- Gen_theta(D, alpha_vec)

  ## Generate topic and word for each word in each document
  doc_list <- vector("list", D)
  
  for (i in 1:D){
    # the length of the document i
    tmp_nd <- nd[i]
  
    # ## a list of vector to contain the results
    # tmp <- WZX_Vec(tmp_nd)
  
    ## Generate W, Z, and X
    wzx_list <- Gen_wzx(i, tmp_nd, phiR, phiS, p_vec, theta)
  
    ## save W, Z, X for document i
    Save_wzx(wzx_list, i, saveDir)
  
    ## Store Created Docs
    doc_list[[i]] <- wzx_list
  
  }
}

show_true_seeds <- function(phiS){
  n <- nrow(phiS)
  seeds_ <- c()
  for(i in 1:n){
    temp <- phiS[i, ]
    seeds_ <- c(seeds_, names(temp[temp > 1e-8]))
  }
  return(seeds_)
}

create_model <- function(data_folder_name, extra_k, 
                                     seed_file_name="", seed_list = NULL,
                                     cull_seed=NULL){

  # cull_seed: Which row (=topic) and column (=number of seed) to use

  set.seed(225)

  folder <- paste0(data_folder, data_folder_name, "/W")
  docs <- list.files(folder, pattern = "*.txt", full.names = TRUE)

  if(seed_file_name != ""){
    seed_file <- paste0(data_folder, data_folder_name, "/", seed_file_name)
    seeddata <- invisible(readr::read_csv(seed_file))
  }

  if(!is.null(cull_seed)){
    seeddata <- seeddata[1:cull_seed[1], 1:cull_seed[2]]
  }

  if(is.null(seed_list)){
    seed_list <- as.list( apply(seeddata, 1, function(x){return(paste0(x, collapse=" "))}) )
  }
  names(seed_list) <- 1:length(seed_list)
  dict <- quanteda::dictionary(seed_list)

  model <- topicdict_model(docs,
               dict = dict, extra_k = extra_k,
               remove_numbers = FALSE, # For simulation, make it false
               remove_punct = TRUE,
               remove_symbols = TRUE,
               remove_separators = TRUE)

  return(model)
}

diagnosis_topic_recovery_heatmap <- function(post, n=25, 
                        topicvec=c(), merge=list()){
  topwords <- top_terms(post, n=n)
  topwords <- data.frame(topwords)
  colnames(topwords) <- paste0("EstTopic", 1:ncol(topwords))

  topwords <- tidyr::gather(topwords, key=EstTopic, value=Word)

  topwords %>%
    mutate(RawWord = Word) %>%
    tidyr::separate(Word,
        into=c("word_id", "TrueTopic"),
        sep="t") %>%
    mutate(TrueTopic = paste0("True", as.character(TrueTopic))) -> res_

  merge_length <- length(merge)
  if(merge_length != 0){
    # Merge Topics
    for(i in 1:merge_length){
      m <- merge[[i]]
      mt <- paste0("True", m)

      res_ %>%
        mutate(TrueTopic=replace(TrueTopic, TrueTopic==mt[1], mt[3])) %>%
        mutate(TrueTopic=replace(TrueTopic, TrueTopic==mt[2], mt[3])) -> res_
    }
  }

  res_ %>%
    group_by(EstTopic, TrueTopic) %>%
    summarise(counts = n()) %>%
    ungroup() %>%
    group_by(EstTopic) %>% 
    mutate(topicsum = sum(counts)) %>%
    ungroup() %>%
    mutate(Proportion = counts / topicsum * 100) -> res_

  num <- length(unique(res_$EstTopic))
  if(is.null(topicvec)){
		res_ %>%
			group_by(EstTopic) %>%
			top_n(1, Proportion) %>%
			arrange(TrueTopic) %>%
			select(EstTopic) -> topicvec 
		topicvec <-	unique(as.integer(gsub("EstTopic", "", topicvec$EstTopic)))
  }else if(length(topicvec) != num){
		message("topicvec length does not match")
		topicvec <- 1:num
  }

  truenum <- length(unique(res_$TrueTopic))

  title <- paste0("Seeded LDA: Top ", as.character(n), " words")

  g <- ggplot(res_, aes(EstTopic, TrueTopic)) +
        geom_tile(aes(fill=Proportion)) + 
        scale_fill_gradient(limits=c(0, 100), low="#e8e8e8", high="#0072B2", name = "Proportion") +
        scale_x_discrete(limits = rev(paste0("EstTopic", topicvec))) +
        coord_flip() +
        # scale_y_discrete(limits = paste0("True", 1:truenum)) +
        xlab("Estimated Topics") + ylab("True Topic") + theme_bw(base_size=13) +
        ggtitle(title) +
        theme(plot.title = element_text(hjust = 0.5))

  return(g)
}
```

## LDA
```{r, warning=FALSE, message=FALSE}
library(tm)

get_lda_result <- function(data_folder_name, iter, k, topicvec=1:k, show_n=25){
  folder <- paste0(data_folder, data_folder_name, "/W")

  # Prepare Data
  corpus <- Corpus(DirSource(folder))
  strsplit_space_tokenizer <- function(x)
      unlist(strsplit(as.character(x), "[[:space:]]+"))

  dtm <- DocumentTermMatrix(corpus,
                           control = list(tokenize=strsplit_space_tokenizer, 
                           stopwords = F, tolower = F, 
                           stemming = F, wordLengths = c(1, Inf)))

  lda <- LDA(dtm, k = k, control = list(seed = 225, iter=iter), method="Gibbs")

  assign <-tidytext::augment(lda, dtm)
  assign %>% 
        mutate(Word=term) %>%
        tidyr::separate(term,
            into=c("word_id", "TrueTopic"),
            sep="t") %>%
        mutate(TrueTopic = paste0("True", as.character(TrueTopic))) %>%
        group_by(.topic, TrueTopic, Word) %>%
        summarise(counts = sum(count)) %>%
        ungroup() %>%
        group_by(.topic) %>%
        top_n(show_n, counts) %>%
        ungroup() %>%
        group_by(.topic, TrueTopic) %>%
        summarise(counts = sum(counts)) %>%
        ungroup() %>%
        group_by(.topic) %>%
        mutate(topicsum = sum(counts)) %>%
        ungroup() %>%
        mutate(Proportion = counts / topicsum * 100,
               EstTopic = paste0("EstTopic", .topic)) -> res

	res %>%
		group_by(EstTopic) %>%
		top_n(1, Proportion) %>%
		arrange(TrueTopic) %>%
		select(EstTopic) -> topicvec 
	topicvec <-	unique(as.integer(gsub("EstTopic", "", topicvec$EstTopic)))

  num <- length(unique(res$EstTopic))
  truenum <- length(unique(res$TrueTopic))
  title <- paste0("LDA: Top ", as.character(show_n), " words")

  g <- ggplot(res, aes(EstTopic, TrueTopic)) +
        geom_tile(aes(fill=Proportion)) + 
        scale_fill_gradient(limits=c(0, 100), low="#e8e8e8", high="#0072B2") +
        scale_x_discrete(limits = rev(paste0("EstTopic", topicvec))) +
        coord_flip() +
        scale_y_discrete(limits = paste0("True", 1:truenum)) +
        xlab("Estimated Topics") + ylab("True Topic") + theme_bw(base_size=13) +
        ggtitle(title) +
        theme(plot.title = element_text(hjust = 0.5))

  return(g)
}
```


# Generate Data
```{r, warning=FALSE, message=FALSE}
data_folder <- tempfile()
name <- "Sim1"
create_sim_data_from_fitted(saveDir=paste0(data_folder, name, "/"), alpha_vec, phiR, phiS, p_original_vec, D=1000, lambda=200)
```

# When LDA gets messed up
* Estimated < True: Ambiguous topics
* True < Estimated: Split topics

```{r, warning=FALSE, message=FALSE, fig.width=9.5, fig.height=6.5}
get_lda_result("Sim1", iter=iter_num, k=6) # True Number of topics

get_lda_result("Sim1", iter=iter_num, k=3) 
get_lda_result("Sim1", iter=iter_num, k=4)
get_lda_result("Sim1", iter=iter_num, k=5)
get_lda_result("Sim1", iter=iter_num, k=7)
get_lda_result("Sim1", iter=iter_num, k=8)
get_lda_result("Sim1", iter=iter_num, k=12)
```

```{r, warning=FALSE, message=FALSE, fig.width=9.5, fig.height=7.5}
get_lda_result("Sim1", iter=iter_num, k=15)
get_lda_result("Sim1", iter=iter_num, k=20)
get_lda_result("Sim1", iter=iter_num, k=30)
get_lda_result("Sim1", iter=iter_num, k=40)
get_lda_result("Sim1", iter=iter_num, k=50)
```

# Compare above with SeededLDA
```{r, warning=FALSE, message=FALSE, fig.width=7, fig.height=5}
# Select seed based on tf-idf
# Explore
folder <- paste0(data_folder, "Sim1", "/W")
docs <- list.files(folder, pattern = "*.txt", full.names = TRUE)
explore_ <- explore(docs,
             remove_numbers = FALSE, # For simulation, make it false
             remove_punct = TRUE,
             remove_symbols = TRUE,
             remove_separators = TRUE)

seed_list <- list(c("w249t1 w833t1 w20t1 w112t1"),
                 c("w593t2 w400t2 w1424t2 w164t2"),
                 c("w629t3 w949t3 w448t3 w77t3"))
explore_$visualize_dict_prop(seed_list)

compare_seededLDA <- function(seed_list, total_k, folder_name="Sim1"){
	extra_k <- total_k - length(seed_list)
	model <- create_model(folder_name, seed_list=seed_list, extra_k=extra_k)
	res <- topicdict_train(model, iter = iter_num)
	post <- topicdict::posterior(res)
	diagnosis_topic_recovery_heatmap(post, 25)
}
```

```{r, warning=FALSE, message=FALSE, fig.width=9.5, fig.height=6.5}
compare_seededLDA(seed_list, total_k=3)
compare_seededLDA(seed_list, total_k=4)
compare_seededLDA(seed_list, total_k=5)
compare_seededLDA(seed_list, total_k=7)
compare_seededLDA(seed_list, total_k=8)
compare_seededLDA(seed_list, total_k=12)
compare_seededLDA(seed_list, total_k=15)
compare_seededLDA(seed_list, total_k=20)
compare_seededLDA(seed_list, total_k=30)
compare_seededLDA(seed_list, total_k=40)
compare_seededLDA(seed_list, total_k=50)
```

# TF-IDF and Proportion
```{r, warning=FALSE, message=FALSE, fig.width=9.5, fig.height=7.5}
data_folder <- "/Users/Shusei/Desktop/temp/Data/Rice500"
docs <- list.files(data_folder, pattern = "*.txt", full.names = TRUE)
explore_ <- explore(docs,
             remove_numbers = TRUE, # For simulation, make it false
             remove_punct = TRUE,
             remove_symbols = TRUE,
             remove_separators = TRUE)
library(ggrepel)
full_data <- dplyr::left_join(explore_$data_tfidf, explore_$data, by=c("term" = "Word"))
ggplot(full_data, aes(x=`Proportion(%)`, y=tf_idf)) +
	geom_point() 
```
