---
title: "Data Structures"
author: "Will Lowe"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Data Structures}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Document Pre-processing

The main function for reading in documents is `topicdict_model`.  
This pre-processes using 
options from the `quanteda` package.  See `help("tokens", package = "quanteda")`
for its options.  By default `topicdict_model` uses `quanteda` to 
guess the file encoding
(if you happen to know the encoding is say UTF-8, then override with 
`encoding = "utf8"`).  The defaults are to lowercase, remove numbers, spaces,
symbols, twitter detritus, and URLs, and not to stem, separate hyphenated words 
or remove stopwords.  To remove stopwords, hand in a character vector, e.g.
`stopwords = quanteda::stopwords("english")`.  To stem, provide a language for
the stemmer, e.g. `stem_language = "english"`.

## Seed words

Seed words are added using a list (or `quanteda::dictionary` which is a list) 
of character vectors.  Each list element
contains the seed words for that topic, and the topic may, but need
not, be named.  

Note that whatever you do by way of preprocessing to the 
texts is also done to the seed words.

## Unseeded topics

In addition to the `length(dict)` seeded topics the function adds
`extra_k` additional unconstrained topics.  

## Model

The `topicdict_model` and `topicdict_train` functions both return a model.  
This is a list with named elements

* `W` a list of vectors of word indexes
* `Z` a list of vectors of topic indicators isomorphic to `W`
* `X` a list of vectors of seed indicators (0/1) isomorphic to `W`
* `vocab` a vector of vocabulary items
* `files` a vector of document filenames
* `dict` a tokenized version of the dictionary
* `seeds` a list of indicators for the seed words in `dict`, organized by topic
* `extra_k` how many unseeded topics will be estimated

`Z` represents which topic a word token was generated by. It is initialized 
randomly from `length(dict) + extra_k` topics, except when `dict` assigns a word to
a particular category, in which case this topic indicator is assigned. For 
example, if a word appears in the second category of `dict` it will always be 
initialized as 1.

`X` is 1 when a word is generated from one of the topics from the dictionary
('seeded') and 0 when it was generated from one of the `extra_k` unseeded topics. 
`X` is initialized randomly except when the word is contained in `dict`.

## Indexing 

All word and topic indicators are zero-based for ease of later processing in 
C++.  For words, `mod$vocab[mod$W + 1]` therefore recovers the 
(tokenized) words of the i-th document from model `mod`.

## Training

the Gibbs sampler is `topicdict_train`.  This function takes a model returned 
by `topicdict_train` or `topicdict_model` and runs `iter` more Gibbs sampling
iterations on it.  This changes the model's `X` and `Z` (in place) but does not
affect the remaing model elements.

## Translation notes

### `phi_s`

When `topicdict_train` runs it extracts `seeds` from the model structure
(a list of character vectors of seed words) and converts each list
element into an 
`std::unordered_map<int, double>` in which the integer key is a zero-starting
index for the seeded topic and the value is the reciprocal of the number of
seed words in that topic, i.e. the (flat) probability of generating each word
if X is 1 for a token.  Each such map is stored in a 
`std::vector< std::unordered_map<int, double> >` called `phi_s`.  This structure
is transient and is reconstructed each time `topicdict_train` is called.

This is how I understand the T&S original code to be working, so please check if
if I've understood it right.  Specifically, there is no constraint that the
number of seeded topics is equal to the number of unconstrained ones, and there
is also no constraint that each seeded topic have the same number of words. If
other parts of the code depend on this assumption they will need to be adjusted.

### `logLik`

I have inserted only one of the original codebase's log likelihood functions.
I'm not sure what the difference between them is, or which one you prefer.
I also notice that there  `NaN`s appear in some components.  Please check this
is not a translation failure on my part.

### Gibbs sampler

For simplicity and to hopefully to make debugging a bit easier, I have 
removed the random selection of a document and token to update in the 
sampling step. The current code simply runs through each document and token
in order and updates `Z` and `X`.

## Model Postprocessing

After the model is complete there is a set of R post-processing functions
based around the `posterior` function.  This constructs some of the working
matrices in `topicdict_train` but does not depend on that function after it is
complete.  

I'm aware that the posterior alpha estimates for each document are not quite
right because they do not have any prior (`alphaK`) information in them.  
This should be fixed when the more important parts (the Gibbs sampler) are 
confirmed to be running correctly.  The same is true of the `beta` parameters.

In general we should probably hand in `gamma1`, `gamma2` etc. rather than 
having them hard coded into `topicdict_train`, but again, that can wait a bit.
