---
title: "Working From the Bara et al. Example"
author: "Will Lowe"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{bara-et-al}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Set Chunk Options

```{r warning=FALSE, message=FALSE}
library(knitr)
knitr::opts_chunk$set(comment = "")
```

## Prepare

```{r, include = FALSE}
library(topicdict)
library(purrr)
library(tidyr)
library(dplyr)
library(ggplot2)
```


We'll use the Bara data, and seeds extracted from the dictionary.  
These are all in package `extdata`:
```{r}
library(quanteda)

doc_folder <- system.file(file.path("extdata", "bara_paras"), package = "topicdict")
seed_file <- system.file(file.path("extdata", "bara_seeds.txt"), package = "topicdict")
## turn this flat file of seeds into a (quanteda dictionary) list as it would be used
seed_list <- Map(function(x){ unlist(strsplit(x, " ")) }, 
                 Filter(function(x){ !grepl(x, pattern = "#") }, # remove comments
                        readLines(seed_file)))
names(seed_list) <- c("debate", "legal", "abortion", "religion", "house", "mother-child")
dict <- dictionary(seed_list)
dict
```

Initialize a model with the default tokenization options, without removing
stopwords or stemming
```{r}
## about 5 seconds, essentially all of it in quanteda's tokenizer
model <- topicdict_model(file.path(doc_folder, "*.txt"), dict)
```
And run a model with one iteration to check everything works
```{r}
model <- topicdict_train(model, iter = 30)
```

Now let's bundle up the posterior and use the new functions on it
```{r}
post <- posterior(model)
top_terms(post, "lift")
# suggest some names for topics
tnames <- suggest_topic_names(post, "lift", n = 4)
tnames
# assign them
post <- set_topic_names(post, tnames)
# top 2 topics in each document
head(top_topics(post))
# and now the document that the most of each topic
top_docs(post)
```



