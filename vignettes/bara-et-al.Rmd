---
title: "Working From the Bara et al. Example"
author: "Will Lowe"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{bara-et-al}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Set Chunk Options

```{r warning=FALSE, message=FALSE}
library(knitr)
knitr::opts_chunk$set(comment = "", eval = FALSE)
```

## Prepare
```{r, include = FALSE}
library(topicdict)
library(purrr)
library(tidyr)
library(dplyr)
library(ggplot2)
```


We'll use the Bara data, and seeds extracted from the dictionary.  These are all in 
package extdata:
```{r, eval = TRUE}
library(quanteda)

folder <- system.file(file.path("extdata", "bara_paras"), package = "topicdict")
docs <- list.files(folder, pattern = "*.txt", full.names = TRUE)
seed_file <- system.file(file.path("extdata", "bara_seeds.txt"), package = "topicdict")
## turn this flat file of seeds into a (quanteda dictionary) list as it would be used
seed_list <- Map(function(x){ unlist(strsplit(x, " ")) }, 
                 Filter(function(x){ !grepl(x, pattern = "#") }, # remove comments
                        readLines(seed_file)))
names(seed_list) <- c("debate", "legal", "abortion", "religion", "house", "mother-child")
dict <- dictionary(seed_list)
```


Initialize a model with some sensible tokenization options
```{r, eval = TRUE}
## about 5 seconds, essentially all of it in quanteda's tokenizer
model <- init(docs,
             dict = dict,
             remove_numbers = TRUE,
             remove_punct = TRUE,
             remove_symbols = TRUE,
             remove_separators = TRUE)
```
And run a model with one iteration to check everything works
```{r}



```

```{r}
#res <- seededlda(data_folder_path = "bara_paras",
#                 seed_csv_path = "bara_seeds.txt",
#                 num_regular_topic = 1,
#                 iter_num = 100,
#                 show_words_num = 20,
#                 full_output = TRUE,
#                 rand_seed = 919)
```

Now let's bundle up the posterior and use the new functions on it
```{r}
post <- posterior(res)
# add document names
post <- set_doc_names(post, docnames)
top_terms(post)
# suggest some names for topics
tnames <- suggest_topic_names(post, n = 4)
tnames
# assign them
post <- set_topic_names(post, tnames)
# top 2 topics in each document
head(top_topics(post, 2))
# and now the document that the most of each topic
top_docs(post)
# we know how people voted
unique(docvars(corp))
# so it's not so surprising that the child related topic
# is represented by the dissenters
top_docs(post, 15)
```

What difference does it make if there are more than one non-seeded topics?

To make things crash, remove `eval=FALSE` from the following code chunks:
```{r, eval = FALSE}
res5 <- seededlda(data_folder_path = "bara_paras",
                  seed_csv_path = "bara_seeds.txt",
                  num_regular_topic = 3,
                  iter_num = 100,
                  show_words_num = 20,
                  full_output = TRUE,
                  rand_seed = 919)
```

```{r, eval = FALSE}
post <- posterior(res)
# add document names
post <- set_doc_names(post, docnames)
top_terms(post)
```

How does this compare to regular LDA?
```{r, eval = FALSE}
library(topicmodels)
dtm_lda <- as.DocumentTermMatrix(dtm_orig)
```





## Appendix: How the seeds were chosen

```{r}
data(corpus_bara_para)
corp <- corpus_subset(corpus_bara_para, 
                      speaker != "Mrs Anne Kerr")

file <- system.file("extdata", "bara-et-al.ykd", package = "topicdict")
ddict <- dictionary(file = file)
ddict_words <- as.vector(unlist(ddict))
# It would be a bad idea to remove gendered pronouns as stop words because 
# of the debate topic, so we don't do that
estops <- stopwords("english") # re-gendered stopwords
# stops minus some gender words
stops <- estops[!(estops %in% c("her", "his", "he", "she", "their", "our"))]
# Make a document term matrix and take a copy that's tf-idf transformed.
# we'll use that to choose informative words.
dtm_orig <- dfm(corp, remove = stops, 
                remove_numbers = TRUE, 
                remove_punct = TRUE,
                remove_symbols = TRUE, 
                remove_separators = TRUE,
                remove_hyphens = TRUE)
tfidf_dtm_orig <- tfidf(dtm_orig)
# The 50 most informative words in thus corpus according to tf-idf score
# sort(colMeans(tfidf_dtm_orig), decreasing = TRUE)[1:50]

informative_wds <- data.frame(tfidf = colMeans(tfidf_dtm_orig))
informative_wds <- informative_wds[order(informative_wds$tfidf, 
                                         decreasing = TRUE), ,drop = FALSE]
# Construct the paper's topic counts using their dictionary.
# This is what Bara et al think the correct, person summed, topic counts ought to be
topics_orig_dict <- dfm(corp, remove = stops, 
                              remove_numbers = TRUE, 
                              remove_punct = TRUE,
                              remove_symbols = TRUE, 
                              remove_separators = TRUE,
                              remove_hyphens = TRUE,
                              dictionary = ddict)
tt <- data.frame(topics_orig_dict, 
                 speaker = docvars(corp, "speaker")) 
colnames(tt) <- unlist(gsub("incoming.txt.", "", colnames(tt)))
tts <- summarise_at(group_by(tt, speaker), 
                    vars(advocacy, legal, medical,
                         moral, procedural, social),
                     sum)
tts <- as.data.frame(tts)
tt_prop <- data.frame(tts[,2:7] / rowSums(tts[,2:7]), 
                      row.names = tts$speaker)
# tt_prop

# Now construct the top 10 seed words using a tf-idf heuristic
# and write to a file
seeds <- lapply(ddict$incoming.txt, function(x){ 
         candidates <- intersect(rownames(informative_wds), x)
         inf <- informative_wds[candidates, ,drop = FALSE]
         rownames(inf)[order(inf$tfidf, decreasing = TRUE)[1:10]]
       })
lines <- as.vector(apply(t(as.data.frame(seeds)), 1, 
                         paste, collapse = " "))
writeLines(c("##", lines), con = file("./bara_seeds.txt"))
## Write out the pre-processed texts to a file
tt <- tokens(corp, remove_numbers = TRUE, 
                   remove_punct = TRUE,
                   remove_symbols = TRUE, 
                   remove_separators = TRUE,
                   remove_hyphens = TRUE)
tt <- lapply(tt, function(x) { 
  x <- char_tolower(x)
  paste(x[!(x %in% stops)], collapse = " ") 
})
names(tt) <- gsub(" ", "_", 
                  paste0(make.unique(docvars(corp, "speaker")), ".txt"))
if (!dir.exists("bara_paras"))
  dir.create("bara_paras")
devnull <- lapply(names(tt), function(x){ 
  writeLines(tt[[x]], con = file.path("bara_paras", x))
})
## assign these document names later
docnames <- names(tt)
```
