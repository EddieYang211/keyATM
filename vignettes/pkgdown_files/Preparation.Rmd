---
title: "Preparation"
output: 
  html_document:
    toc: false
---

## Preparing Texts
To fit topic models with **keyATM**, users need to prepare texts for `keyATM_read()`. `keyATM_read()` has three ways to input texts. If you want to check an example with a built-in dataset, please proceed to [the next section](Preparation.html#reading-documents-into-keyatm).

### Using quanteda dfm (**recommended**)

**keyATM** can read a `dfm` object created by **quanteda** package (**this method is strongly recommended**). Since **keyATM** does not provide preprocessing functions, we recommend users to preprocess documents with **quanteda**. 
By making a `token` object from a `corpus` object, **quanteda** can perform various preprocessing methods ([quanteda Quick Start: Tokenizing texts](https://quanteda.io/articles/quickstart.html#tokenizing-texts)).

We show a concrete example of preprocessing data and loading it into the **keyATM**. 
We use the US presidential inaugural address data that is one of the built-in datasets of the **quanteda**.
```{r, eval=FALSE, warning=FALSE, message=FALSE, fig.align='center'}
library(keyATM)
library(quanteda)
library(magrittr)
data(data_corpus_inaugural, package = "quanteda")
```

`tokens` and related functions in the **quanteda** provide various preprocessing functions. Preprocessing can reduce the number of unique features (words) in the corpus, which is critical for increasing the interpretability. In the example below, we adopt some of the most common preprocessing steps.


```{r, eval=FALSE, warning=FALSE, message=FALSE, fig.align='center'}
data_tokens <- tokens(data_corpus_inaugural,
                      remove_numbers = TRUE, 
                      remove_punct = TRUE, 
                      remove_symbols = TRUE,
                      remove_separators = TRUE,
                      remove_url = TRUE) %>%
                 tokens_tolower() %>%
                 tokens_remove(c(stopwords("english"), 
                               "may", "shall", "can",
                               "must", "upon", "with", "without")) %>%
                 tokens_select(min_nchar = 3)
```
`tokens` removes punctuations and unnecessary characters, 
`tokens_tolower` convert all characters into lower cases,
`tokens_remove` remove general stop words (e.g, the, is, at, with `stopwords("english")`)
and corpus specific high frequent words ("may"", "shall"", ..., "without" in this example), 
and `tokens_select` drops
short words that do not usually contribute to interpreting topics.



Here is an example preprocessing steps with **quanteda**. In this example, we read texts with **readtext** package.
```{r, eval=FALSE, warning=FALSE, message=FALSE, fig.align='center'}
library(quanteda)
library(readtext)

# Read text files
raw_docs <- readtext("PATH_TO_THE_FOLDER/*.txt",
                     encoding = "UTF-8")

# Preprocessing with quanteda and create a dfm object
key_corpus <- corpus(raw_docs, text_field = "text")

# If you use the covariate model, please consider using `docvars` argument
key_corpus <- corpus(raw_docs, text_field = "text", docvars = COVARIATES)

key_tokens <- tokens(key_corpus,
                     remove_numbers = TRUE, 
                     remove_punct = TRUE, 
                     remove_symbols = TRUE,
                     remove_separators = TRUE,
                     remove_url = TRUE) %>%
                tokens_tolower() %>%
                tokens_remove(stopwords("english")) %>%
                tokens_select(min_nchar = 3)

key_dfm <- dfm(key_tokens) %>%
             dfm_trim(min_termfreq = 5, min_docfreq = 2)

# Read texts into keyATM
keyATM_docs <- keyATM_read(key_dfm)
```

You can also use `c(stopwords("english"), "your", "stopwords")` if you want to add corpus specific stopwords.

### Alternative ways to read texts
There are two other ways to read texts, which we do not recommend. 
Please make sure to preprocess texts with other packages or softwares. 
In both methods, each word should be separated by a single space. 

Using data.frame or tibble:
`keyATM_read()` can read `data.frame` and `tibble` if you preprocess texts without **quanteda**. Please store texts in a column named `text`.

```
> head(docs)  # `docs` stores preprocessed texts
# A tibble: 6 x 1
  text                                                                      
  <chr>                                                                     
1 h.r h.r one hundred first congress congress congress united u...
2 first congress one congress congress united united state stae...
3 one one one one one one one one one one one one one one one o...
4 h.r h.r one one one hundred hundred first first congress cong...
5 congress congress one united united united united united unit...
6 h.r h.r one one one one one hundred hundred first congress co...
```

```{r, eval=F, warning=FALSE, message=FALSE}
# Read texts into keyATM
keyATM_docs <- keyATM_read(docs)
```


Reading directly from files:
If you have preprocessed text files, you can pass a list of files to `keyATM_read()`.

```{r, eval=F, warning=FALSE, message=FALSE, fig.align='center'}
# Create a list of paths to text files
textfiles <- list.files(PATH_TO_THE_FOLDER, pattern = "*.txt", full.names = TRUE)

# Read texts into keyATM
keyATM_docs <- keyATM_read(textfiles)
```

## Reading documents into keyATM
Before fitting the model, we need to read texts and keywords into **keyATM**. Here we use **quanteda**'s `dfm`.

**keyATM** package includes a tidy dataset for illustration purpose. In this example, we use text data of the Congressional Bills scraped from [congress.gov](https://www.congress.gov/) (in total 140 documents). You can load this data by `data(keyATM_data_bills)`.
```{r, warning=FALSE, message=FALSE, fig.align='center'}
library(quanteda)
library(keyATM)
data(keyATM_data_bills)
bills_dfm <- keyATM_data_bills$doc_dfm  # quanteda dfm object
keyATM_docs <- keyATM_read(bills_dfm)
```

## Preparing keywords

### Keywords list
Keywords should be in a list. For example, we prepare four keyword-topics. Keywrods should be stored in a list. Each element in the list is a character vector, which corresponds to a single keyword-topic. The number of keywords can vary between topics. 

```{r, warning=FALSE, message=FALSE, fig.align='center'}
bills_keywords <- list(
                       Education = c("education", "child", "student"),
                       Law       = c("court", "law", "attorney"),
                       Health    = c("public", "health"),
                       Drug      = c("drug")
                      )
```


### Checking keywords
```{r, warning=FALSE, message=FALSE, fig.align='center', height=3, width=3}
visualize_keywords(keyATM_docs, bills_keywords)
```
The figure helps you to check the frequency of keywords. Including low-frequency keywords do not help the model in general. **keyATM** automatically prune keywords that do not appear in the documents.

Proportion is defined as a number of times a keyword occurs in the corpus divided by the total length of documents. This measures the frequency of the keyword in the corpus. Formally, the proportion of the keyword $v$ is,
$$
\begin{align*}
    \text{Proportion of }v = \frac{\sum_{d=1}^{D} \sum_{i=1}^{N_d} I(w_{di} = v) }{\sum_{d=1}^{D} N_d}
\end{align*}
$$
where $N_d$ is the length of the document $d$ and $I$ is an indicator function. Keywords of each toopic are ordered by the proportion (x-axsis).


## Next Step
Now you have texts and keywords! The next step is to fit a model `keyATM()`. **keyATM** has three models:

* [keyATM Base](keyATM_base.html)
  * This is an extension of the most famous topic model, Latent Dirichlet Allocation.
  * If you do not have covariates, this model is your first option.
* [keyATM Covariates](keyATM_cov.html)
  * If you have covariates, please use this model.
  * This model uses covariates to model topic prevalence (the prior of document-topic distribution).
* [keyATM Dynamic](keyATM_dynamic.html)
  * If you want to explicitly consider time structure, please use this model.


You can find details in [FAQ](FAQ.html#which-model-to-use).


