---
title: "Progress Report"
author: "Shusei Eshima"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{GenSimDataLDA}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r warning=FALSE, message=FALSE, echo = FALSE}
# package_folder <- "/Users/Shusei/Dropbox/Study/Project/ImaiText/topicdict"
# setwd(package_folder)
# devtools::document() ; devtools::install() ; library(topicdict)
library(knitr)
knitr::opts_chunk$set(comment = "")
```


```{r, warning=FALSE, message=FALSE}
library(topicdict)
library(quanteda)
library(tibble)
library(ggplot2)
library(dplyr)
library(topicmodels)
library(hashmap)
library(tm)
library(purrr)
library(clue)
iter_num <- 50
extra_k <- 2
```


```{r, warning=FALSE, message=FALSE}
lda_initialization <-  function(files, dict, extra_k = 1, encoding = "UTF-8",
                            lowercase = TRUE,
                            remove_numbers = TRUE, remove_punct = TRUE,
                            remove_symbols = TRUE, remove_separators = TRUE,
                            remove_twitter = FALSE, remove_hyphens = FALSE,
                            remove_url = TRUE, stem_language = NULL,
                            stopwords = NULL,
                            alpha = 50/(length(dict) + extra_k),
                            beta = 0.01, beta_s = 0.1,
                            gamma_1 = 1.0, gamma_2 = 1.0){
  cl <- match.call()

  proper_len <- length(dict) + extra_k
  if (length(alpha) == 1){
    message("All ", proper_len, " values for alpha starting as ", alpha)
    alpha = rep(alpha, proper_len)
  } else if (length(alpha) != proper_len)
    stop("Starting alpha must be a scalar or a vector of length ", proper_len)

  # Prepare Data
	# Run lda
  corpus <- Corpus(DirSource(folder))
  strsplit_space_tokenizer <- function(x)
      unlist(strsplit(as.character(x), "[[:space:]]+"))

  dtm <- DocumentTermMatrix(corpus,
                           control = list(tokenize=strsplit_space_tokenizer, 
                           stopwords = F, tolower = T, 
                           stemming = F, wordLengths = c(1, Inf)))

  lda <- LDA(dtm, k = proper_len+extra_k, control = list(seed = 225, iter=iter_num), method="Gibbs")
	lda_z <- lda@z # Get lda topic assignments

  args <- list(remove_numbers = remove_numbers,
               remove_punct = remove_punct,
               remove_symbols = remove_symbols,
               remove_separators = remove_separators,
               remove_twitter = remove_twitter,
               remove_hyphens = remove_hyphens,
               remove_url = remove_url)

  if ("corpus" %in% class(files)){
    args$x <- files
	}else {
    ## preprocess each text
    ## for debugging
	  # Use files <- list.files(doc_folder, pattern="txt", full.names=T) when you pass
    df <- data.frame(text = unlist(lapply(files, function(x){ paste0(readLines(x, encoding = encoding),
                                                              collapse = "\n") })),
                     stringsAsFactors = FALSE)
    df$doc_id <- paste0("text", 1:nrow(df))
    args$x <- corpus(df)
    ## debugging until here
  }

  # args$x <- corpus(readtext(file_pattern, encoding = encoding))
  # for new version quanteda, you need this
  args$x$documents$doc_id <- paste0("text", 1:nrow(df))
  doc_names <- docvars(args$x, "doc_id") # docnames
  toks <- do.call(tokens, args = args)
  if (lowercase)
    toks <- tokens_tolower(toks)
  if (!is.null(stopwords))
    toks <- tokens_remove(toks, stopwords)
  if (!is.null(stem_language))
    toks <- tokens_wordstem(toks, language = stem_language)

  ## apply the same preprocessing to the seed words
  args$x <- do.call(rbind, lapply(as.list(dict), paste0, collapse = " "))
  dtoks <- do.call(tokens, args = args)
  if (lowercase)
    dtoks <- tokens_tolower(dtoks)
  if (!is.null(stopwords))
    dtoks <- tokens_remove(dtoks, stopwords)
  if (!is.null(stem_language))
    dtoks <- tokens_wordstem(dtoks, language = stem_language)
  K <- length(dtoks) # number of seeded categories a.k.a. size of dictionary

  ## construct W and a vocab list (W elements are 0 based ids)
  wd_names <- attr(toks, "types") # vocab
  wd_map <- hashmap(wd_names, as.integer(1:length(wd_names) - 1))
  W <- lapply(toks, function(x){ wd_map[[x]] })

  # zx_assigner maps seed words to category ids
  seed_wdids <- unlist(lapply(dtoks, function(x){ wd_map$find(x) }))
  cat_ids <- rep(1:K - 1, unlist(lapply(dtoks, length)))
  zx_assigner <- hashmap(as.integer(seed_wdids), as.integer(cat_ids))

	## xx indicates whether the word comes from a seed topic-word distribution or not
	make_x <- function(x){
    seeded <- as.numeric(zx_assigner$has_keys(x)) # 1 if they're a seed
		# Use x structure
    x[seeded == 0] <- 0 # non-seeded words have x=0
		x[seeded == 1] <- sample(0:1, length(x[seeded == 1]), prob = c(0.3, 0.7), replace = TRUE)
			# seeded words have x=1 probabilistically
    x
	}

  X <- lapply(W, make_x)

  # if the word is a seed, assign the appropriate (0 start) Z, else a random Z
	doc_len <- map(W, length) %>% unlist()
	doc_names <- names(doc_len)
	lda_z_split <- split(lda_z, rep(1:length(doc_names), doc_len)) # lda z assignment by documents
	names(lda_z_split) <- doc_names

  make_z_key <- function(x){
		zz <- zx_assigner[[x]] # if it is a seed word, we already know the topic
    zz
  }
  Z <- lapply(W, make_z_key) # only for a keyworded word

	make_z <- function(key, lda_z){
		key[is.na(key)] <- lda_z[is.na(key)]
		return(key)
	}
	Z <- mapply(make_z, Z, lda_z_split)

  # dictionary category names -> vector of word_id.
  # (Later processes ignore names)
  seeds <- lapply(dtoks, function(x){ wd_map$find(x) })
  names(seeds) <- names(dict)

  ll <- list(W = W, Z = Z, X = X, vocab = wd_names,
             files = doc_names, dict = dtoks, seeds = seeds, extra_k = extra_k,
             alpha = alpha, gamma_1 = gamma_1, gamma_2 = gamma_2,
             beta = beta, beta_s = beta_s, call = cl,
						 alpha_iter = list(), model_fit = list(),
						 call = cl)
  class(ll) <- c("topicdict", class(ll))
  ll
}

```

```{r, warning=FALSE, message=FALSE}
lda_initialization2 <-  function(files, dict, extra_k = 1, encoding = "UTF-8",
                            lowercase = TRUE,
                            remove_numbers = TRUE, remove_punct = TRUE,
                            remove_symbols = TRUE, remove_separators = TRUE,
                            remove_twitter = FALSE, remove_hyphens = FALSE,
                            remove_url = TRUE, stem_language = NULL,
                            stopwords = NULL,
                            alpha = 50/(length(dict) + extra_k),
                            beta = 0.01, beta_s = 0.1,
                            gamma_1 = 1.0, gamma_2 = 1.0){
  cl <- match.call()

  proper_len <- length(dict) + extra_k
  if (length(alpha) == 1){
    message("All ", proper_len, " values for alpha starting as ", alpha)
    alpha = rep(alpha, proper_len)
  } else if (length(alpha) != proper_len)
    stop("Starting alpha must be a scalar or a vector of length ", proper_len)

  # Prepare Data
	# Run lda
  corpus <- Corpus(DirSource(folder))
  strsplit_space_tokenizer <- function(x)
      unlist(strsplit(as.character(x), "[[:space:]]+"))

  dtm <- DocumentTermMatrix(corpus,
                           control = list(tokenize=strsplit_space_tokenizer, 
                           stopwords = F, tolower = T, 
                           stemming = F, wordLengths = c(1, Inf)))

  lda <- LDA(dtm, k = proper_len+extra_k, control = list(seed = 225, iter=iter_num), method="Gibbs")
	lda_z <- lda@z # Get lda topic assignments

  args <- list(remove_numbers = remove_numbers,
               remove_punct = remove_punct,
               remove_symbols = remove_symbols,
               remove_separators = remove_separators,
               remove_twitter = remove_twitter,
               remove_hyphens = remove_hyphens,
               remove_url = remove_url)

  if ("corpus" %in% class(files)){
    args$x <- files
	}else {
    ## preprocess each text
    ## for debugging
	  # Use files <- list.files(doc_folder, pattern="txt", full.names=T) when you pass
    df <- data.frame(text = unlist(lapply(files, function(x){ paste0(readLines(x, encoding = encoding),
                                                              collapse = "\n") })),
                     stringsAsFactors = FALSE)
    df$doc_id <- paste0("text", 1:nrow(df))
    args$x <- corpus(df)
    ## debugging until here
  }

  # args$x <- corpus(readtext(file_pattern, encoding = encoding))
  # for new version quanteda, you need this
  args$x$documents$doc_id <- paste0("text", 1:nrow(df))
  doc_names <- docvars(args$x, "doc_id") # docnames
  toks <- do.call(tokens, args = args)
  if (lowercase)
    toks <- tokens_tolower(toks)
  if (!is.null(stopwords))
    toks <- tokens_remove(toks, stopwords)
  if (!is.null(stem_language))
    toks <- tokens_wordstem(toks, language = stem_language)

  ## apply the same preprocessing to the seed words
  args$x <- do.call(rbind, lapply(as.list(dict), paste0, collapse = " "))
  dtoks <- do.call(tokens, args = args)
  if (lowercase)
    dtoks <- tokens_tolower(dtoks)
  if (!is.null(stopwords))
    dtoks <- tokens_remove(dtoks, stopwords)
  if (!is.null(stem_language))
    dtoks <- tokens_wordstem(dtoks, language = stem_language)
  K <- length(dtoks) # number of seeded categories a.k.a. size of dictionary

  ## construct W and a vocab list (W elements are 0 based ids)
  wd_names <- attr(toks, "types") # vocab
  wd_map <- hashmap(wd_names, as.integer(1:length(wd_names) - 1))
  W <- lapply(toks, function(x){ wd_map[[x]] })

  # zx_assigner maps seed words to category ids
  seed_wdids <- unlist(lapply(dtoks, function(x){ wd_map$find(x) }))
  cat_ids <- rep(1:K - 1, unlist(lapply(dtoks, length)))
  zx_assigner <- hashmap(as.integer(seed_wdids), as.integer(cat_ids))

	## xx indicates whether the word comes from a seed topic-word distribution or not
	make_x <- function(x){
    seeded <- as.numeric(zx_assigner$has_keys(x)) # 1 if they're a seed
		# Use x structure
    x[seeded == 0] <- 0 # non-seeded words have x=0
		x[seeded == 1] <- sample(0:1, length(x[seeded == 1]), prob = c(0.3, 0.7), replace = TRUE)
			# seeded words have x=1 probabilistically
    x
	}

  X <- lapply(W, make_x)

  # if the word is a seed, assign the appropriate (0 start) Z, else a random Z
	doc_len <- map(W, length) %>% unlist()
	doc_names <- names(doc_len)
	lda_z_split <- split(lda_z, rep(1:length(doc_names), doc_len)) # lda z assignment by documents
	names(lda_z_split) <- doc_names

  weight_z <- table(lda_z)/length(lda_z)
  phi <- exp(lda@beta)
  colnames(phi) <- lda@terms
  phi <- phi[, sort(colnames(phi))]
  
  # choose keywords per topic
  match_vec <- as.numeric(length(dict))
  dict_unlist <- lapply(dict, function(x) {unlist(strsplit(x, " "))})
  # matrix to contain the "weight"
  match_mat <- matrix(0, nrow = max(sapply(dict_unlist, length)), ncol = nrow(phi))
  for (i in 1:length(dict)){
    terms <- unlist(strsplit(dict[[i]], " "))
    phi_key <- phi[, terms]
    phi_key <- sweep(phi_key, 2, apply(phi_key, 2, sum), FUN = "/")
    # row is the keyword topic and column is the LDA output to match
    match_mat[i, ] <-  apply(phi_key, 1, sum) * weight_z
    # match_vec[i] <- id
  }
  ## mm is the match based on the hungarian algorithm
  ## each id of the vector mm denotes key word topics (i)
  ## each item of the vector mm denotes estimated LDA topics (mm[i])
  mm <- solve_LSAP(match_mat, maximum = TRUE)
  
  lda_z_split2 <- lapply(lda_z_split, function(x){ ifelse(x > 0, 0, 1)  })
  
  ## input keywords topics
  for (i in 1:length(mm)){
    for (d in 1:length(lda_z_split)){
      lda_z_split2[[d]][lda_z_split[[d]] == mm[i]] <- i-1
    }
  }
  
  ## topic that does not used for keyword topics
  non_key_topics <- c(1:(proper_len+extra_k))[-mm]
  # non_key_topics_slda <- c(  )
  
  ## assign LDA output topics that does not used for keyword topics
  # for (i in non_key_topics){
  #   for (d in 1:length(lda_z_split)){
  #     lda_z_split2[[d]][lda_z_split[[d]] == non_key_topics_lda ] <- 
  #   }
  # }
  # 
  K <- length(dict)
  unused_topics <- (K):(K+extra_k-1)
  for(i in 1:length(non_key_topics)){
    original <- non_key_topics[i]
    changed <- unused_topics[i]
    for (d in 1:length(lda_z_split)){
      lda_z_split2[[d]][lda_z_split[[d]] == original ] <- changed 
    }
  }

  Z <- lda_z_split2

  # dictionary category names -> vector of word_id.
  # (Later processes ignore names)
  seeds <- lapply(dtoks, function(x){ wd_map$find(x) })
  names(seeds) <- names(dict)

  ll <- list(W = W, Z = Z, X = X, vocab = wd_names,
             files = doc_names, dict = dtoks, seeds = seeds, extra_k = extra_k,
             alpha = alpha, gamma_1 = gamma_1, gamma_2 = gamma_2,
             beta = beta, beta_s = beta_s, call = cl,
						 alpha_iter = list(), model_fit = list(),
						 call = cl)
  class(ll) <- c("topicdict", class(ll))
  ll
}

```


```{r, warning=FALSE, message=FALSE}
create_model_from_lda <- function(folder, extra_k, 
                                     seed_file_name="", seed_list = NULL,
                                     cull_seed=NULL, initializer="normal"){

  # cull_seed: Which row (=topic) and column (=number of seed) to use

  set.seed(225)

  docs <- list.files(folder, pattern = "*.txt", full.names = TRUE)

  if(seed_file_name != ""){
    seed_file <- paste0(data_folder, data_folder_name, "/", seed_file_name)
    seeddata <- invisible(readr::read_csv(seed_file))
  }

  if(!is.null(cull_seed)){
    seeddata <- seeddata[1:cull_seed[1], 1:cull_seed[2]]
  }

  if(is.null(seed_list)){
    seed_list <- as.list( apply(seeddata, 1, function(x){return(paste0(x, collapse=" "))}) )
  }
  names(seed_list) <- 1:length(seed_list)
  dict <- quanteda::dictionary(seed_list)

	if(initializer=="normal"){
		model <- topicdict_model(docs,
								 dict = dict, extra_k = extra_k,
								 remove_numbers = FALSE, # For simulation, make it false
								 remove_punct = TRUE,
								 remove_symbols = TRUE,
								 remove_separators = TRUE)
	}else{
		model <- lda_initialization2(docs,
								 dict = dict, extra_k = extra_k,
								 remove_numbers = FALSE, # For simulation, make it false
								 remove_punct = TRUE,
								 remove_symbols = TRUE,
								 remove_separators = TRUE)	
	}


  return(model)
}

```


```{r, warning=FALSE, message=FALSE}
compare_seededLDA <- function(seed_list, total_k, folder_name="Sim1", initializer="normal"){
	extra_k <- total_k - length(seed_list)
	folder <- paste0(data_folder, folder_name, "/W")
	files <- list.files(folder, pattern = "*.txt", full.names = TRUE)
	model <- create_model(folder, seed_list=seed_list, extra_k=extra_k, initializer=initializer)
	res <- topicdict_train(model, iter = iter_num)
	post <- topicdict::posterior(res)
	return(post)
}
```


```{r, warning=FALSE, message=FALSE}
compare_seededLDA(seed_list, 8, initializer="t")
```
