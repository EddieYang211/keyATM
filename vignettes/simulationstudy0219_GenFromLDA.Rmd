---
title: "Generate a simulation dataset from LDA results"
author: "Shusei Eshima"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{GenSimDataLDA}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r warning=FALSE, message=FALSE, echo = FALSE}
library(knitr)
knitr::opts_chunk$set(comment = "")
```


```{r, warning=FALSE, message=FALSE}
library(topicdict)
library(quanteda)
library(tibble)
library(ggplot2)
library(dplyr)
library(topicmodels)
iter_num <- 20
extra_k <- 2
```

# Fit Seeded LDA
We'll use the Bara data, and seeds extracted from the dictionary.  
These are all in package `extdata`:
```{r, warning=FALSE, message=FALSE}
doc_folder <- system.file(file.path("extdata", "bara_paras"), package = "topicdict")
seed_file <- system.file(file.path("extdata", "bara_seeds.txt"), package = "topicdict")

## turn this flat file of seeds into a (quanteda dictionary) list as it would be used
seed_list <- Map(function(x){ unlist(strsplit(x, " ")) }, 
                 Filter(function(x){ !grepl(x, pattern = "#") }, # remove comments
                        readLines(seed_file)))

# Original topic names from Bara dictionary
names(seed_list) <- c("advocacy", "legal", "medical", "moral", "procedural", "social")
dict <- dictionary(seed_list)
dict
```

Initialize a model with the default tokenization options, without removing
stopwords or stemming, and run the model.
```{r, warning=FALSE, message=FALSE}
set.seed(225)

stops <- setdiff(c(stopwords("english"), letters), 
                  c("he", "his", "him", "himself", "she", "hers", "her", "herself"))
model <- topicdict_model(file.path(doc_folder, "*.txt"), extra_k=extra_k,
                         dict, stopwords = stops)
res <- topicdict_train(model, iter = 500)
```

# Create Data 
## Organize fitted data
```{r, warning=FALSE, message=FALSE}
collapse <- function(obj){
  temp <- unlist(obj) 
  names(temp) <- NULL
  return(temp)
}
W <- collapse(res$W)
Z <- collapse(res$Z)
X <- collapse(res$X)
fit <- tibble(W=W, Z=Z, X=X)


# phiR
fit %>%
  filter(X == 0) %>%
  group_by(W, Z) %>%
  summarize(count_wordtopic = n()) %>%
  ungroup() %>%
  group_by(Z) %>%
  mutate(count_topicsum = sum(count_wordtopic)) %>%
  ungroup() %>%
  mutate(phi_r = count_wordtopic / count_topicsum) %>%
  select(W, Z, phi_r) %>%
  tidyr::spread(W, phi_r, fill=1e-12) %>%
  select(-Z) -> fitted

phiR <- t(apply(fitted%>%as.matrix(), 1, function(x)( return(x / sum(x)) )))

# phiS
fit %>%
  filter(X == 1) %>%
  group_by(W, Z) %>%
  summarize(count_wordtopic = n()) %>%
  ungroup() %>%
  group_by(Z) %>%
  mutate(count_topicsum = sum(count_wordtopic)) %>%
  ungroup() %>%
  mutate(phi_s = count_wordtopic / count_topicsum) %>%
  select(W, Z, phi_s) %>%
  tidyr::spread(W, phi_s, fill=1e-12) %>%
  select(-Z) -> fitted

phiS <- t(apply(fitted%>%as.matrix(), 1, function(x)( return(x / sum(x)) )))


# p
fit %>% 
  group_by(X, Z) %>%
  summarize(count_xtopic = n()) %>%
  ungroup() %>%
  group_by(Z) %>%
  mutate(count_topicsum = sum(count_xtopic)) %>%
  ungroup() %>%
  mutate(p = count_xtopic / count_topicsum) %>%
  filter(X==1) %>%
  select(p) %>% as.matrix() %>% as.vector() -> p_original_vec

# alpha
alpha_vec <- res$alpha
```

## Preparation for creating simulation data 
```{r, warning=FALSE, message=FALSE}
rbern <- function(n, prob){
  return(rbinom(n, 1, prob))
}
rcat <- function(n, p){
  if (is.vector(p)) {
    x <- as.vector(which(rmultinom(n, size=1, prob=p) == 1, arr.ind=TRUE)[, "row"])
  } else {
    d <- dim(p)
    n <- d[1]
    k <- d[2]
    lev <- dimnames(p)[[2]]
    if (!length(lev)) lev <- 1:k
    z <- colSums(p)
    U <- apply(p, 1, cumsum)
    U[,k] <- 1
    un <- rep(runif(n), rep(k,n))
    x <- lev[1 + colSums(un > U)]}
    return(x)
}
## From MCMCpack
rdirichlet <- function(n, alpha) {
  l <- length(alpha)
  x <- matrix(rgamma(l*n,alpha), ncol=l, byrow=TRUE)
  sm <- x %*% rep(1,l)
  return(x / as.vector(sm))
}

CreateDir_wzx <- function(saveDir){
  dir1 <- paste0(saveDir, "W")
  dir.create(dir1)
  dir2 <- paste0(saveDir, "Z")
  dir.create(dir2)
  dir3 <- paste0(saveDir, "X")
  dir.create(dir3)
  return(list(dir1, dir2, dir3))
}

Gen_ND <- function(document, lambda){
  nd <- numeric(document)
  ## generate document length
  nd <- rpois(document, lambda)
  return(nd)
}

Gen_theta <- function(doc_len, alpha_vec){
  theta <- rdirichlet(doc_len, alpha_vec)
  return(theta)
}

Gen_z <- function(theta, d, doc_len){
  z <- rcat(doc_len, theta[d,])
  return(z)
}

Gen_x <- function(topic_num, probX){
  x <- rbern(1, probX[topic_num])
  return(x)
}

Gen_w <- function(phi, topic){
  tmp <- rcat(1, phi[topic,])
  words <- colnames(phi)
  return(words[tmp])
}

Gen_w_seeds <- function(index, phiR, phiS, z, x){
  topic <- z[index]
  indicator <- x[index]

  if (indicator == 0){
    # Regular words
    word <- Gen_w(phiR, topic)
    word <- paste0("W", as.character(word), "T", as.character(topic))
  } else {
    # Seed words
    word <- Gen_w(phiS, topic)
    word <- paste0("W", as.character(word), "T", as.character(topic))
  }
  return(word)
}

Gen_wzx <- function(doc_id, doc_len, phiR, phiS, p_vec, theta){
  z <- Gen_z(theta, doc_id, doc_len)
  x <- sapply(z, Gen_x, probX=p_vec)
  w <- sapply(1:doc_len, Gen_w_seeds, phiR=phiR, phiS=phiS, z=z, x=x)
  # print(w)
  return(list(doc_id=rep(doc_id, doc_len), w=w,z=z,x=x))
}

Write_wzx <- function(content, name, saveDir){
  txt <- paste(as.character(content), collapse = " ")
  name <- paste0(saveDir, name, ".txt")
  write(txt, name)
}

Save_wzx <- function(data, i, saveDir){
  ## save W for document i

  Write_wzx(data[["w"]], paste0("text", i), paste0(saveDir,"W/"))

  ## save Z for document i
  Write_wzx(data[["z"]], paste0("z_", i), paste0(saveDir,"Z/"))

  ## save X for document i
  Write_wzx(data[["x"]], paste0("x_", i), paste0(saveDir,"X/"))
}

create_sim_data_from_fitted <- function(
  saveDir, alpha, phiR, phiS, p,
  D=200, lambda=300, rand_seed=123
){
  # Set Seed
  set.seed(rand_seed)

  # Prepare Directory
  dir.create(saveDir)
  Dirs <- CreateDir_wzx(saveDir)

  # Set the number of topics
  K <- nrow(phiS)
  phiR <- phiR[1:K, ]
  alpha_vec <- alpha[1:K]
  V <- ncol(phiR)
  p_vec <- p[1:K]

  # Length of the documents
  nd <- Gen_ND(D, lambda)

  # Get theta
  theta <- Gen_theta(D, alpha_vec)

  ## Generate topic and word for each word in each document
  doc_list <- vector("list", D)
  
  for (i in 1:D){
    # the length of the document i
    tmp_nd <- nd[i]
  
    # ## a list of vector to contain the results
    # tmp <- WZX_Vec(tmp_nd)
  
    ## Generate W, Z, and X
    wzx_list <- Gen_wzx(i, tmp_nd, phiR, phiS, p_vec, theta)
  
    ## save W, Z, X for document i
    Save_wzx(wzx_list, i, saveDir)
  
    ## Store Created Docs
    doc_list[[i]] <- wzx_list
  
  }
}

show_true_seeds <- function(phiS){
  n <- nrow(phiS)
  seeds_ <- c()
  for(i in 1:n){
    temp <- phiS[i, ]
    seeds_ <- c(seeds_, names(temp[temp > 1e-8]))
  }
  return(seeds_)
}

create_model <- function(data_folder_name, extra_k, 
                                     seed_file_name="", seed_list = NULL,
                                     cull_seed=NULL){

  # cull_seed: Which row (=topic) and column (=number of seed) to use

  set.seed(225)

  folder <- paste0(data_folder, data_folder_name, "/W")
  docs <- list.files(folder, pattern = "*.txt", full.names = TRUE)

  if(seed_file_name != ""){
    seed_file <- paste0(data_folder, data_folder_name, "/", seed_file_name)
    seeddata <- invisible(readr::read_csv(seed_file))
  }

  if(!is.null(cull_seed)){
    seeddata <- seeddata[1:cull_seed[1], 1:cull_seed[2]]
  }

  if(is.null(seed_list)){
    seed_list <- as.list( apply(seeddata, 1, function(x){return(paste0(x, collapse=" "))}) )
  }
  names(seed_list) <- 1:length(seed_list)
  dict <- quanteda::dictionary(seed_list)

  model <- topicdict_model(docs,
               dict = dict, extra_k = extra_k,
               remove_numbers = FALSE, # For simulation, make it false
               remove_punct = TRUE,
               remove_symbols = TRUE,
               remove_separators = TRUE)

  return(model)
}

diagnosis_topic_recovery_heatmap <- function(post, n=25, 
                        topicvec=c(), merge=list()){
  topwords <- top_terms(post, n=n)
  topwords <- data.frame(topwords)
  colnames(topwords) <- paste0("EstTopic", 1:ncol(topwords))

  topwords <- tidyr::gather(topwords, key=EstTopic, value=Word)

  topwords %>%
    mutate(RawWord = Word) %>%
    tidyr::separate(Word,
        into=c("word_id", "TrueTopic"),
        sep="t") %>%
    mutate(TrueTopic = paste0("True", as.character(TrueTopic))) -> res_

  merge_length <- length(merge)
  if(merge_length != 0){
    # Merge Topics
    for(i in 1:merge_length){
      m <- merge[[i]]
      mt <- paste0("True", m)

      res_ %>%
        mutate(TrueTopic=replace(TrueTopic, TrueTopic==mt[1], mt[3])) %>%
        mutate(TrueTopic=replace(TrueTopic, TrueTopic==mt[2], mt[3])) -> res_
    }
  }

  res_ %>%
    group_by(EstTopic, TrueTopic) %>%
    summarise(counts = n()) %>%
    ungroup() %>%
    group_by(EstTopic) %>% 
    mutate(topicsum = sum(counts)) %>%
    ungroup() %>%
    mutate(Proportion = counts / topicsum * 100) -> res_

  num <- length(unique(res_$EstTopic))
  if(is.null(topicvec)){
    topicvec <- 1:num
  }else if(length(topicvec) != num){
    message("Topicvec length does not match with the topic number")
    topicvec <- 1:num
  }

  truenum <- length(unique(res_$TrueTopic))

  title <- paste0("Seeded LDA: Top ", as.character(n), " words")

  g <- ggplot(res_, aes(EstTopic, TrueTopic)) +
        geom_tile(aes(fill=Proportion)) + 
        scale_fill_gradient(limits=c(0, 100), low="#e8e8e8", high="#0072B2") +
        scale_x_discrete(limits = rev(paste0("EstTopic", topicvec))) +
        coord_flip() +
        # scale_y_discrete(limits = paste0("True", 1:truenum)) +
        xlab("Estimated Topics") + ylab("True Topic") + theme_bw(base_size=13) +
        ggtitle(title) +
        theme(plot.title = element_text(hjust = 0.5))

  return(g)
}
```

## LDA
```{r, warning=FALSE, message=FALSE}
library(topicmodels)
library(tidytext)
library(tm)

get_lda_result <- function(data_folder_name, iter, k, show_n=25){
  folder <- paste0(data_folder, data_folder_name, "/W")

  # Prepare Data
  corpus <- Corpus(DirSource(folder))
  strsplit_space_tokenizer <- function(x)
      unlist(strsplit(as.character(x), "[[:space:]]+"))

  dtm <- DocumentTermMatrix(corpus,
                           control = list(tokenize=strsplit_space_tokenizer, 
                           stopwords = F, tolower = F, 
                           stemming = F, wordLengths = c(1, Inf)))

  lda <- LDA(dtm, k = k, control = list(seed = 225, iter=iter), method="Gibbs")

  assign <- augment(lda, dtm)
  assign %>% 
        mutate(Word=term) %>%
        separate(term,
                into=c("True", "raw_word_id"),
                sep="S|R") %>%
        mutate(TrueTopic = paste0("True", as.character(True))) %>%
        group_by(.topic, TrueTopic, Word) %>%
        summarise(counts = sum(count)) %>%
        ungroup() %>%
				group_by(.topic) %>%
        top_n(show_n, counts) %>%
				ungroup() %>%
				group_by(.topic, TrueTopic) %>%
				summarise(counts = sum(counts)) %>%
        ungroup() %>%
				group_by(.topic) %>%
				mutate(topicsum = sum(counts)) %>%
				ungroup() %>%
        mutate(Proportion = counts / topicsum * 100,
               EstTopic = paste0("EstTopic", .topic)) -> res

  num <- length(unique(res$EstTopic))
  truenum <- length(unique(res$TrueTopic))
  title <- paste0("LDA: Top ", as.character(show_n), " words")

  g <- ggplot(res, aes(EstTopic, TrueTopic)) +
        geom_tile(aes(fill=Proportion)) + 
        scale_fill_gradient(limits=c(0, 100), low="#e8e8e8", high="#0072B2") +
        scale_x_discrete(limits = rev(paste0("EstTopic", 1:num))) +
        coord_flip() +
        scale_y_discrete(limits = paste0("True", 1:truenum)) +
        xlab("Estimated Topics") + ylab("True Topic") + theme_bw(base_size=13) +
        ggtitle(title) +
        theme(plot.title = element_text(hjust = 0.5))

  return(g)
}
```


# Generate Data
```{r, warning=FALSE, message=FALSE}
data_folder <- tempfile()
name <- "Sim1"
create_sim_data_from_fitted(saveDir=paste0(data_folder, name, "/"), alpha_vec, phiR, phiS, p_original_vec, D=1000, lambda=200)
```

# Use Simulation Data 1
```{r, warning=FALSE, message=FALSE, fig.width=6, fig.height=4}
# Explore
folder <- paste0(data_folder, "Sim1", "/W")
docs <- list.files(folder, pattern = "*.txt", full.names = TRUE)
explore_ <- explore(docs,
             remove_numbers = FALSE, # For simulation, make it false
             remove_punct = TRUE,
             remove_symbols = TRUE,
             remove_separators = TRUE)
explore_$top_words()
explore_$show_words("w65t5")

seed_list <- list(c("w33t5 w65t5 w209t5"),
                 c("w223t2 w536t2 w541t2"),
                 c("w6t3 w0t3 w26t3"),
                 c("w11t1 w64t1 w273t1"),
                 c("w134t4 w92t4 w832t4"))
model <- create_model("Sim1", seed_list=seed_list, extra_k=1)
res <- topicdict_train(model, iter = iter_num)
post <- topicdict::posterior(res)
new_order <- c(4,2,3,5,1,6)
```


## Analysis
### Modelfit
```{r, warning=FALSE, message=FALSE, fig.width=6, fig.height=4}
diagnosis_model_fit(post, start=2)
```

### Heatmap
EstTopic6 is a regular topic.
```{r, warning=FALSE, message=FALSE, fig.width=10, fig.height=6.5}
diagnosis_topic_recovery_heatmap(post, 25, topicvec=new_order)
```

### Recovery of p
EstTopic6 is a regular topic.
```{r, warning=FALSE, message=FALSE, fig.width=7, fig.height=5.5}
diagnosis_p(post, new_order)
round(p_original_vec*100, 3)
```

# Use Simulation Data 2 (Merge)
```{r, warning=FALSE, message=FALSE}
# Explore
seed_list <- list(c("w33t5 w65t5 w223t2 w536t2"),
                 c("w6t3 w0t3 w26t3"),
                 c("w11t1 w64t1 w273t1"),
                 c("w134t4 w92t4 w832t4"),
                 c("w173t6 w185t6 w651t6"))
model <- create_model("Sim1", seed_list=seed_list, extra_k=1)
res <- topicdict_train(model, iter = iter_num)
post <- topicdict::posterior(res)
```

## Analysis
### Modelfit
```{r, warning=FALSE, message=FALSE, fig.width=6, fig.height=4}
diagnosis_model_fit(post, start=2)
```

### Heatmap
EstTopic6 is a regular topic.
```{r, warning=FALSE, message=FALSE, fig.width=10, fig.height=6.5}
diagnosis_topic_recovery_heatmap(post, 25, topicvec=c(3,1,6,2,4,5), merge=list(c(2,5,"2_5")))
```


# Use Simulation Data 3 (Split)
```{r, warning=FALSE, message=FALSE}
# Explore
seed_list <- list(c("w33t5 w65t5"),
                 c("w118t5 w107t5 w19t5"),
                 c("w6t3 w0t3 w26t3"),
                 c("w11t1 w64t1 w273t1"),
                 c("w134t4 w92t4 w832t4"),
                 c("w173t6 w185t6 w651t6"))
model <- create_model("Sim1", seed_list=seed_list, extra_k=1)
res <- topicdict_train(model, iter = iter_num)
post <- topicdict::posterior(res)
new_order <- c(4,7,3,5,1,2,6)
```

## Analysis
### Modelfit
```{r, warning=FALSE, message=FALSE, fig.width=6, fig.height=4}
diagnosis_model_fit(post, start=2)
```

### Heatmap
EstTopic7 is a regular topic.
```{r, warning=FALSE, message=FALSE, fig.width=10, fig.height=6.5}
diagnosis_topic_recovery_heatmap(post, 25, topicvec=new_order)
```

### Recovery of p
EstTopic7 is a regular topic.
```{r, warning=FALSE, message=FALSE, fig.width=7, fig.height=5.5}
diagnosis_p(post)
```

### Best LDA result (true number of topics)
```{r, warning=FALSE, message=FALSE, fig.width=10, fig.height=6.5}
get_lda_result("Sim1", iter=iter_num, k=6)
```
